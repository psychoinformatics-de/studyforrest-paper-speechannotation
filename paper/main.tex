% For more detailed article preparation guidelines, please see:
% http://f1000research.com/author-guidelines
\documentclass[10pt,a4paper,onecolumn]{article}
\usepackage{f1000_styles}
\usepackage{units}
\geometry{right=4cm}
% enable the 'endfloat' package to sort all tables and figures to the
% end of the document without having to edit the manuscript
%\usepackage{endfloat}
\usepackage{booktabs}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
%% Default: numerical citations
\usepackage[numbers]{natbib}

%% Uncomment this lines for superscript citations instead
% \usepackage[super]{natbib}

%% Uncomment these lines for author-year citations instead
% \usepackage[round]{natbib}
% \let\cite\citep

\begin{document}
\input{descr-stats-anno.tex}
\input{descr-stats-regressors.tex}

\title{A studyforrest extension, an annotation of spoken language in the German dubbed movie ``Forrest Gump'' and its audio-description}
%\titlenote{whatever titlenote here}

\author[1, 2]{Christian~O.~Häusler \\ email \href{mailto:c.haeusler@fz-juelich.de}{c.haeusler@fz-juelich.de} }
\author[1, 2]{Michael~Hanke \\ email \href{mailto:m.hanke@fz-juelich.de}{m.hanke@fz-juelich.de} }

\affil[1]{Institute of Neuroscience and Medicine, Brain \& Behaviour (INM-7), Research Centre Jülich, Jülich, 52425, Germany}
\affil[2]{Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University, Düsseldorf, 40225, Germany}

\maketitle

\todo[inline]{wie email und ORCID (du: https://orcid.org/0000-0001-6398-6370; ich: https://orcid.org/0000-0002-0936-317X) unterbringen?}

\todo{Wort zur Validierungsstudie; Es sind erst 110 von max 300 Worten
verbraucht} \thispagestyle{fancy} \begin{abstract}
% up to 300 words
Here we present an annotation of speech in the audio-visual movie ``Forrest
Gump'' and its audio-description (i.e. the audio-only variant of the movie) as
an addition to a large public functional brain imaging dataset
(\href{www.studyforrest.org}{studyforrest.org}). The annotation provides
information about the exact timing of each of the more than 2500 spoken
sentences, 16000 words (including 202 non-speech vocalizations), 66000 phonemes,
and their corresponding speaker. Additionally for every word, we provide
lemmatization, a simple part-of-speech-tagging (15 grammatical categories), a
detailed part-of-speech tagging (43 grammatical categories), syntactic
dependencies, and a semantic analysis based on word embeddings which represents
each word in a 300-dimensional semantic space. This annotation enables
researchers to create sophisticated models of brain activity related to
phonetics, grammar, syntax, and semantics under conditions of real-life
complexity.  \end{abstract}


\section*{Keywords}
% maximal 8
annotation, language, speech, narrative, natural stimulation, fMRI, studyforrest

\listoftodos

\clearpage

\section*{Introduction}
% intro to intro
Cognitive and psychiatric neuroimaging are moving towards studying brain
functions under conditions of lifelike complexity
\citep{sonkusare2019naturalistic, eickhoff2020towards}. Motion pictures
\citep{hasson2008neurocinematics} and continuous narratives \citep{honey2012not,
lerner2011topographic} are increasingly utilized as so called ``naturalistic
stimuli''.

% annotation bottleneck
Naturalistic stimuli are usually designed for commercial purposes and to
entertain their audience. Thus, the temporal structure of their feature space is
usually not explicitly known leading to an ``annotation bottleneck'' when used
for research purposes \citep{aliko2020naturalistic}.
% data-driven methods
Data-driven methods like intersubject correlation
(ISC)\citep{hasson2004intersubject} or independent component analysis
(ICA)\citep{bartels2004chronoarchitecture} are often used to analyze
corresponding fMRI data circumventing this bottleneck. However, data-driven
methods alone fall usually short of pinpointing their results to specific
stimulus events \citep{kauttonen2015optimizing}.
% model-driven methods
Model-driven methods, like a general linear model (GLM), can be useful to test
hypotheses on specific brain functions under more ecologically valid conditions,
to statistically control confounding stimulus features, and to explain not just
``how'' the brain is responding to a stimulus but also ``why''
\citep{hamilton2018revolution}.
% GLM: example studies
For example, studies using GLMs based on annotations of a stimulus' temporal
structure have elucidated how the brain responds to visual features of a movie
\citep{bartels2004mapping} or speech-related features of a narrative
\citep{rocca2019language}.
% combination of data-driven and model-driven approaches
Additionally, stimulus annotations can inform data-driven methods about a
stimulus' temporal dynamics, or model-driven and data-driven methods can be
combined to improve interpretability of results \citep{lahnakoski2012stimulus}.

% we:
For this publication, we revised previous, preliminary annotations done by our
lab and now provide one single annotation with exact onset and offset of each
sentence, word and phoneme (see Table \ref{tab:overview} for an overview) spoken
in the audio-visual movie ``Forrest Gump'' \citep{ForrestGumpMovie} and its
audio-description \citep{ForrestGumpGermanAD}.
% the studyforrest dataset
fMRI data of participants watching the audio-visual movie
\citep{hanke2016simultaneous} and listening to the audio-description
\citep{hanke2014audiomovie} are the core data of the publicly available
\textit{studyforrest} dataset (\href{www.studyforrest.org}{studyforrest.org}).
% why even bother?
The current publication enables researchers to model hemodynamic brain responses
that correlate with a variety of aspects of spoken language ranging from a
speaker's identity, to phonetics, grammar, syntax, and semantics.
% extending previous non-speech annotations
The current annotation of speech extends already available annotations of
portrayed emotions \citep{labs2015portrayed}, perceived emotions
\citep{lettieri2019emotionotopy}, as well as cuts and locations depicted in the
movie \citep{haeusler2016annotation}. All annotations can be used in any study
focusing on aspects of real-life cognition by serving as additional confound
measures describing the the temporal structure and feature space of the stimuli.


\section*{Materials and methods} \subsection*{Stimulus}
% stimulus != DVD but ``research cut''
We annotated speech in the slightly shortened ``research cut'' of the movie
\citep{hanke2016simultaneous} and its temporally aligned audio-description
\citep{hanke2014audiomovie}.
% movie source
For the movie stimulus, we used the audio track of the German DVD release
\citep{ForrestGumpDVD}.
% audio source
For the audio-only stimulus, we used the movie's audio-description that was
broadcast as an additional audio track for visually impaired listeners on Swiss
public television \citep{ForrestGumpGermanAD}.
% diff in audio AV vs. AO
The plot of the movie is already carried by an off-screen voice of the main
character Forrest Gump. In the largely identical audio-description, a male
narrator additionally describes essential aspects of the visual scenery when
there is no off-screen voice, dialog, or other relevant auditory content.

\subsection*{Annotation procedure}
% preliminary annotations
Preliminary, manual orthographic transcriptions of dialogs, non-speech
vocalizations (e.g. laughter or groaning) and the script for the
audio-description's narrator were merged and converted to Praat's
\citep{boersma2019praat} TextGrid format.
% preparing annotation in Praat before forced alignment
% \href{www.fon.hum.uva.nl/praat}{fon.hum.uva.nl/praat}.
The merged transcription containing a rough onset and offset for usually a
couple of sentences was edited in Praat for manual validation against the
actual content of the audio material.
% steps conducted
The following steps were done in several passes to iteratively improve the
quality of the data: approximate temporal onsets and offsets were corrected, and
intervals containing several sentences were split into intervals containing only
one sentence.
% dropping of sentences hard to understand
When two or more persons were speaking simultaneously the less dominant voice
was dropped. Low volume background speech (especially occurring during music or
continuous environmental noise) or low volume non-speech vocalizations which
were subjectively assessed to be incomprehensible for the audience were dropped,
too.

% intro for forced aligner
We then used the Montreal Forced Aligner \citep{mcauliffe2017montreal} to
algorithmically identify the exact onset of offset of each sentence, and the
onset and offset of each word and phoneme.
% preparing other inputs before forced alignment pronunciation dictionary
% http://prosodylab.org
% https://raw.githubusercontent.com/prosodylab/prosodylab.dictionaries/master/de.dict
To enable the aligner to look up the phonemes embedded within each word, we
chose a German pronunciation dictionary provided by Prosodylab \citep
{gorman2011prosodylab} that uses the Prosodylab PhoneSet to describe the
pronunciation of phonemes.
% extending the dictionary with formerly unknown German words
To improve the detection rate of the automatic alignment, the dictionary was
manually updated with German words that occur in the stimuli but were originally
missing in the dictionary.
% \href{http://mlmlab.org/mfa/dictionaries/english.dict}{mlmlab.org/mfa/dictionaries/english.dict}
% English words in the otherwise German stimulus
The pronunciation of English words and phonemes occurring in the otherwise
German audio track was taken from an English pronunciation dictionary (following
the ARPAbet PhoneSet) provided by the Montreal Language Modeling lab
(\href{http://mlmlab.org/mfa/dictionaries/english.dict}{mlmlab.org}).
% performing the forced alignment audio input to the aligner
The audio track was converted from FLAC to WAV via FFmpeg \citep{ffmpeg} to meet
the aligner's requirements.
% inputs
This WAV file, the merged transcription, and the updated dictionary were
submitted to the aligner that first trained an acoustic model on the data and
then performed the alignment.

% output
The resulting .TextGrid file that contains onsets and offsets of words and
phonemes was corrected manually and iteratively in several passes using Praat:
% onset/offset checking & correction
In a first step, onsets and offsets on which the automatic alignment performed
moderately were corrected.
% handling poor performance
Some low volume sentences that are spoken in continuously noisy settings (e.g.
during battle or hurricane) were removed due to poor overall alignment
performance.
% adding whole sentences
In a second step, the complete sentences of the orthographic transcription were
copied into the annotation created by the aligner.
% speaker identity
In a third step, a speaker's identity was added for each sentence (see Table
\ref{tab:speakers} for the most often occurring speakers).
% constant on-the-fly checking of previous
During every step results from previous steps were constantly checked again for
errors and further improvements.

\begin{table*}[tbp]
    \caption{Overview of the annotation's content for the audio-only stimulus
that comprises the additional narrator. Counts are given for the whole stimulus
(\texttt{all}) and its individual segments used during fMRI scanning.
The category \texttt{sentences} comprises complete grammatical sentences which
are additionally marked in the annotation with a full stop at the end (``my feet hurt.'').
It also comprises questions (``do you want a chocolate?''), exclamations (``run
away!''), or non-speech vocalizations in quick succession (``ha, ha, ha''), or
in isolation (e.g. ``Forrest?'', ``Forrest!'', ``ha'') at time points when
speakers switch rapidly. The category \texttt{words} comprises each word or
non-speech vocalization (N=\aPosNonspeechAll) in isolation.}
\label{tab:overview}
\begin{tabular}{llllllllll}
\toprule
\textbf{category} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}\\
\midrule
sentences  & \aSentencesAll & \aSentencesI & \aSentencesII & \aSentencesIII & \aSentencesIV & \aSentencesV & \aSentencesVI & \aSentencesVII & \aSentencesVIII \tabularnewline
words  & \aWordsAll & \aWordsI & \aWordsII & \aWordsIII & \aWordsIV & \aWordsV & \aWordsVI & \aWordsVII & \aWordsVIII \tabularnewline
phonemes  & \aPhonesAll & \aPhonesI & \aPhonesII & \aPhonesIII & \aPhonesIV & \aPhonesV & \aPhonesVI & \aPhonesVII & \aPhonesVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[btp]
\caption{Sentences spoken by the ten most often occurring speakers sorted alphabetically. The narrator only occurs in the audio-description of the movie.
Overall 97 persons were identified.
Names are mostly identical to the names used in \citep{labs2015portrayed}.}
\label{tab:speakers}
\begin{tabular}{llllllllll}
\toprule
\textbf{name} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}\\
\midrule
Bubba  & \aBubbaAll & \aBubbaI & \aBubbaII & \aBubbaIII & \aBubbaIV & \aBubbaV & \aBubbaVI & \aBubbaVII & \aBubbaVIII \tabularnewline
Forrest  & \aForrestAll & \aForrestI & \aForrestII & \aForrestIII & \aForrestIV & \aForrestV & \aForrestVI & \aForrestVII & \aForrestVIII \tabularnewline
Forrest (child)  & \aForrestchildAll & \aForrestchildI & \aForrestchildII & \aForrestchildIII & \aForrestchildIV & \aForrestchildV & \aForrestchildVI & \aForrestchildVII & \aForrestchildVIII \tabularnewline
Forrest (v.o.)  & \aForrestvoAll & \aForrestvoI & \aForrestvoII & \aForrestvoIII & \aForrestvoIV & \aForrestvoV & \aForrestvoVI & \aForrestvoVII & \aForrestvoVIII \tabularnewline
Hancock  & \aHancockAll & \aHancockI & \aHancockII & \aHancockIII & \aHancockIV & \aHancockV & \aHancockVI & \aHancockVII & \aHancockVIII \tabularnewline
Jenny  & \aJennyAll & \aJennyI & \aJennyII & \aJennyIII & \aJennyIV & \aJennyV & \aJennyVI & \aJennyVII & \aJennyVIII \tabularnewline
Jenny (child)  & \aJennychildAll & \aJennychildI & \aJennychildII & \aJennychildIII & \aJennychildIV & \aJennychildV & \aJennychildVI & \aJennychildVII & \aJennychildVIII \tabularnewline
Lt. Dan  & \aLtdanAll & \aLtdanI & \aLtdanII & \aLtdanIII & \aLtdanIV & \aLtdanV & \aLtdanVI & \aLtdanVII & \aLtdanVIII \tabularnewline
Mrs. Gump  & \aMrsgumpAll & \aMrsgumpI & \aMrsgumpII & \aMrsgumpIII & \aMrsgumpIV & \aMrsgumpV & \aMrsgumpVI & \aMrsgumpVII & \aMrsgumpVIII \tabularnewline
Narrator  & \aNarratorAll & \aNarratorI & \aNarratorII & \aNarratorIII & \aNarratorIV & \aNarratorV & \aNarratorVI & \aNarratorVII & \aNarratorVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


% natural language processing (NLP); Tiger Treebank Corpus:
% \href{https://www.ims.uni-stuttgart.de}{ims.uni-stuttgart.de}.
We employed the Python package spaCy \citep{spacy2} and its accompanying German
language model (\texttt{de\_core\_news\_md}; trained on the TIGER
Treebank corpus \citep{brants2004tiger}) to automatically analyze linguistic
features of each word in their corresponding sentence.
% dropping of non-speech vocalizations because they often delude the algo
Non-speech vocalizations were dropped from the sentences before analysis to
improve results.
% performed analyses
We then performed analyses regarding part-of-speech (i.e. grammatical tagging or
word-category disambiguation), syntactic dependencies, lemmatization, word
embedding (i.e. a multi-dimensional meaning representation of a word), and if
the word is one of the most common words of the German language (i.e. if the
word is part of a stop list).


\subsection*{Data Legend}
% TextGrid vs. TSV
The annotation is available in two different versions, both providing the same
information: a) as a text-based Praat TextGrid file and b) as a text-based,
tab-separated value (TSV) formatted table.
% preview of following subsubsections
The following descriptions refer to the ten columns of the TSV file, namely
\texttt{onset}, \texttt{duration}, \texttt{person}, \texttt{text}, \texttt{pos}, \texttt{tag}, \texttt{dep}, \texttt{lemma},
\texttt{stop}, \texttt{vector}.


\subsubsection*{Start (\texttt{start})} The onset of the sentence, word or
phoneme. Time stamps are provided in the format seconds.milliseconds of stimulus
onset.


\subsubsection*{Duration (\texttt{duration})} The duration of the sentence,
word or phoneme provided the format seconds.milliseconds.


\subsubsection*{Speaker identity (\texttt{person})} Name of the person that
speaks the sentence, word or phoneme. See Table \ref{tab:speakers} for the ten
most often occurring speakers.


\subsubsection*{Text (\texttt{text})}
% ``Großphoneme'': n<200
The text of a spoken sentence or word, or the pronunciation of a phoneme.
Phonemes of German words follow the Prosodylab PhoneSet, English words follow
the ARPAbet PhoneSet.


\subsubsection*{Simple part-of-speech tag (\texttt{pos})} A simple
part-of-speech tagging (grammatical tagging; word-category disambiguation) of
words.
% annotation scheme
The tag labels of this simple part-of-speech tagging follow the Universal
Dependencies v2 POS tag set
(\href{https://universaldependencies.org}{universaldependencies.org}).
% reference to table
See Table \ref{tab:pos} for a description of the labels and the respective
counts of all 15 labels. Nouns that spaCy mistook for proper nouns or vice versa
were corrected via script.
% s. list at the beginning of script code/add_part-of-speech-tagging2textgrid.py
% flags for sentences and phonemes in POS column
Additionally in cells of this column, sentences are tagged as \texttt{SENTENCE},
and phonemes are tagged as \texttt{PHONEME} to facilitate filtering in potential
further processing steps.

\begin{table*}[t]
\caption{Simple part-of-speech tagging (\texttt{pos}) performed by the Python package spaCy \citep{spacy2}.
All 15 labels sorted alpapetically.
Descriptions were taken from spaCy.explain().
Non-speech vocalizations (\texttt{NONSPEECH}) were manually identified.
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:pos}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
ADJ & \aPosAdj & \aPosAdjAll & \aPosAdjI & \aPosAdjII & \aPosAdjIII & \aPosAdjIV & \aPosAdjV & \aPosAdjVI & \aPosAdjVII \tabularnewline
ADP & \aPosAdp & \aPosAdpAll & \aPosAdpI & \aPosAdpII & \aPosAdpIII & \aPosAdpIV & \aPosAdpV & \aPosAdpVI & \aPosAdpVII & \aPosAdpVIII \tabularnewline
ADV & \aPosAdv & \aPosAdvAll & \aPosAdvI & \aPosAdvII & \aPosAdvIII & \aPosAdvIV & \aPosAdvV & \aPosAdvVI & \aPosAdvVII & \aPosAdvVIII \tabularnewline
AUX & \aPosAux & \aPosAuxAll & \aPosAuxI & \aPosAuxII & \aPosAuxIII & \aPosAuxIV & \aPosAuxV & \aPosAuxVI & \aPosAuxVII & \aPosAuxVIII \tabularnewline
CONJ & \aPosConj & \aPosConjAll & \aPosConjI & \aPosConjII & \aPosConjIII & \aPosConjIV & \aPosConjV & \aPosConjVI & \aPosConjVII & \aPosConjVIII \tabularnewline
DET & \aPosDet & \aPosDetAll & \aPosDetI & \aPosDetII & \aPosDetIII & \aPosDetIV & \aPosDetV & \aPosDetVI & \aPosDetVII & \aPosDetVIII \tabularnewline
NONSPEECH & non-speech vocalization & \aPosNonspeechAll & \aPosNonspeechI & \aPosNonspeechII & \aPosNonspeechIII & \aPosNonspeechIV & \aPosNonspeechV & \aPosNonspeechVI & \aPosNonspeechVII & \aPosNonspeechVIII \tabularnewline
NOUN & \aPosNoun & \aPosNounAll & \aPosNounI & \aPosNounII & \aPosNounIII & \aPosNounIV & \aPosNounV & \aPosNounVI & \aPosNounVII & \aPosNounVIII \tabularnewline
NUM & \aPosNum & \aPosNumAll & \aPosNumI & \aPosNumII & \aPosNumIII & \aPosNumIV & \aPosNumV & \aPosNumVI & \aPosNumVII & \aPosNumVIII \tabularnewline
PART & \aPosPart & \aPosPartAll & \aPosPartI & \aPosPartII & \aPosPartIII & \aPosPartIV & \aPosPartV & \aPosPartVI & \aPosPartVII & \aPosPartVIII \tabularnewline
PRON & \aPosPron & \aPosPronAll & \aPosPronI & \aPosPronII & \aPosPronIII & \aPosPronIV & \aPosPronV & \aPosPronVI & \aPosPronVII & \aPosPronVIII \tabularnewline
PROPN & \aPosPropn & \aPosPropnAll & \aPosPropnI & \aPosPropnII & \aPosPropnIII & \aPosPropnIV & \aPosPropnV & \aPosPropnVI & \aPosPropnVII & \aPosPropnVIII \tabularnewline
SCONJ & \aPosSconj & \aPosSconjAll & \aPosSconjI & \aPosSconjII & \aPosSconjIII & \aPosSconjIV & \aPosSconjV & \aPosSconjVI & \aPosSconjVII & \aPosSconjVIII \tabularnewline
VERB & \aPosVerb & \aPosVerbAll & \aPosVerbI & \aPosVerbII & \aPosVerbIII & \aPosVerbIV & \aPosVerbV & \aPosVerbVI & \aPosVerbVII & \aPosVerbVIII \tabularnewline
X & \aPosX & \aPosXAll & \aPosXI & \aPosXII & \aPosXIII & \aPosXIV & \aPosXV & \aPosXVI & \aPosXVII & \aPosXVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Detailed part-of-speech tag (\texttt{tag})}
% \href{https://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/germantagsets}{Stuttgart-Tübingen-Tagset}
A detailed part-of-speech tagging of words following the TIGER Treebank
annotation scheme \citep{brants2004tiger} which is based on the
Stuttgart-Tübingen-Tagset \citep{schiller1999stts}.
% reference to table
See Table \ref{tab:tag} for a descripion  of the labels and the respective
counts of the 15 most often occuring labels (overall 43 labels). Nouns that
spaCy mistook for proper nouns or vice versa were corrected via script.
% s. list at the beginning of script code/add_part-of-speech-tagging2textgrid.py


\begin{table*}[t]
\caption{Detailed part-of-speech tagging (\texttt{tag}) performed by the Python package spaCy \citep{spacy2}.
The 15 most often occurring labels (overall 43 labels) sorted alphabetically.
Descriptions were taken from spaCy.explain().
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:tag}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
ADJA & \aTagAdja & \aTagAdjaAll & \aTagAdjaI & \aTagAdjaII & \aTagAdjaIII & \aTagAdjaIV & \aTagAdjaV & \aTagAdjaVI & \aTagAdjaVII & \aTagAdjaVIII \tabularnewline
ADJD & \aTagAdjd & \aTagAdjdAll & \aTagAdjdI & \aTagAdjdII & \aTagAdjdIII & \aTagAdjdIV & \aTagAdjdV & \aTagAdjdVI & \aTagAdjdVII & \aTagAdjdVIII \tabularnewline
ADV & \aTagAdv & \aTagAdvAll & \aTagAdvI & \aTagAdvII & \aTagAdvIII & \aTagAdvIV & \aTagAdvV & \aTagAdvVI & \aTagAdvVII & \aTagAdvVIII \tabularnewline
APPR & \aTagAppr & \aTagApprAll & \aTagApprI & \aTagApprII & \aTagApprIII & \aTagApprIV & \aTagApprV & \aTagApprVI & \aTagApprVII & \aTagApprVIII \tabularnewline
ART & \aTagArt & \aTagArtAll & \aTagArtI & \aTagArtII & \aTagArtIII & \aTagArtIV & \aTagArtV & \aTagArtVI & \aTagArtVII & \aTagArtVIII \tabularnewline
KON & \aTagKon & \aTagKonAll & \aTagKonI & \aTagKonII & \aTagKonIII & \aTagKonIV & \aTagKonV & \aTagKonVI & \aTagKonVII & \aTagKonVIII \tabularnewline
NE & \aTagNe & \aTagNeAll & \aTagNeI & \aTagNeII & \aTagNeIII & \aTagNeIV & \aTagNeV & \aTagNeVI & \aTagNeVII & \aTagNeVIII \tabularnewline
NN & \aTagNn & \aTagNnAll & \aTagNnI & \aTagNnII & \aTagNnIII & \aTagNnIV & \aTagNnV & \aTagNnVI & \aTagNnVII & \aTagNnVIII \tabularnewline
PPER & \aTagPper & \aTagPperAll & \aTagPperI & \aTagPperII & \aTagPperIII & \aTagPperIV & \aTagPperV & \aTagPperVI & \aTagPperVII & \aTagPperVIII \tabularnewline
PPOSAT & \aTagPposat & \aTagPposatAll & \aTagPposatI & \aTagPposatII & \aTagPposatIII & \aTagPposatIV & \aTagPposatV & \aTagPposatVI & \aTagPposatVII & \aTagPposatVIII \tabularnewline
PTKVZ & \aTagPtkvz & \aTagPtkvzAll & \aTagPtkvzI & \aTagPtkvzII & \aTagPtkvzIII & \aTagPtkvzIV & \aTagPtkvzV & \aTagPtkvzVI & \aTagPtkvzVII & \aTagPtkvzVIII \tabularnewline
VAFIN & \aTagVafin & \aTagVafinAll & \aTagVafinI & \aTagVafinII & \aTagVafinIII & \aTagVafinIV & \aTagVafinV & \aTagVafinVI & \aTagVafinVII & \aTagVafinVIII \tabularnewline
VVFIN & \aTagVvfin & \aTagVvfinAll & \aTagVvfinI & \aTagVvfinII & \aTagVvfinIII & \aTagVvfinIV & \aTagVvfinV & \aTagVvfinVI & \aTagVvfinVII & \aTagVvfinVIII \tabularnewline
VVINF & \aTagVvinf & \aTagVvinfAll & \aTagVvinfI & \aTagVvinfII & \aTagVvinfIII & \aTagVvinfIV & \aTagVvinfV & \aTagVvinfVI & \aTagVvinfVII & \aTagVvinfVIII \tabularnewline
VVPP & \aTagVvpp & \aTagVvppAll & \aTagVvppI & \aTagVvppII & \aTagVvppIII & \aTagVvppIV & \aTagVvppV & \aTagVvppVI & \aTagVvppVII & \aTagVvppVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Syntactic dependency (\texttt{dep})} Information about a word's
syntactic dependencies with other words within the same sentence.
% annotation scheme
Information follows the TIGER Treebank annotation scheme \citep{brants2004tiger}
and is given in the format: ``arc label;word's head;word's child1, word's
child2, ...'', where the ``arc label'' (see Table \ref{tab:dep}) describes the
type of syntactic relation that connects a "child" (the word) to its
``head''.

\begin{table*}[t]
\caption{Syntactic dependencies (\texttt{dep}) performed by the Python package spaCy \citep{spacy2}.
The 15 most often occuring labels (overall 37 labels) sorted alphabetically.
Descriptions were taken from spaCy.explain().
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:dep}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
cd & \aDepCd & \aDepCdAll & \aDepCdI & \aDepCdII & \aDepCdIII & \aDepCdIV & \aDepCdV & \aDepCdVI & \aDepCdVII & \aDepCdVIII \tabularnewline
cj & \aDepCj & \aDepCjAll & \aDepCjI & \aDepCjII & \aDepCjIII & \aDepCjIV & \aDepCjV & \aDepCjVI & \aDepCjVII & \aDepCjVIII \tabularnewline
cp & \aDepCp & \aDepCpAll & \aDepCpI & \aDepCpII & \aDepCpIII & \aDepCpIV & \aDepCpV & \aDepCpVI & \aDepCpVII & \aDepCpVIII \tabularnewline
da & \aDepDa & \aDepDaAll & \aDepDaI & \aDepDaII & \aDepDaIII & \aDepDaIV & \aDepDaV & \aDepDaVI & \aDepDaVII & \aDepDaVIII \tabularnewline
ju & \aDepJu & \aDepJuAll & \aDepJuI & \aDepJuII & \aDepJuIII & \aDepJuIV & \aDepJuV & \aDepJuVI & \aDepJuVII & \aDepJuVIII \tabularnewline
mnr & \aDepMnr & \aDepMnrAll & \aDepMnrI & \aDepMnrII & \aDepMnrIII & \aDepMnrIV & \aDepMnrV & \aDepMnrVI & \aDepMnrVII & \aDepMnrVIII \tabularnewline
mo & \aDepMo & \aDepMoAll & \aDepMoI & \aDepMoII & \aDepMoIII & \aDepMoIV & \aDepMoV & \aDepMoVI & \aDepMoVII & \aDepMoVIII \tabularnewline
nk & \aDepNk & \aDepNkAll & \aDepNkI & \aDepNkII & \aDepNkIII & \aDepNkIV & \aDepNkV & \aDepNkVI & \aDepNkVII & \aDepNkVIII \tabularnewline
oa & \aDepOa & \aDepOaAll & \aDepOaI & \aDepOaII & \aDepOaIII & \aDepOaIV & \aDepOaV & \aDepOaVI & \aDepOaVII & \aDepOaVIII \tabularnewline
oc & \aDepOc & \aDepOcAll & \aDepOcI & \aDepOcII & \aDepOcIII & \aDepOcIV & \aDepOcV & \aDepOcVI & \aDepOcVII & \aDepOcVIII \tabularnewline
pd & \aDepPd & \aDepPdAll & \aDepPdI & \aDepPdII & \aDepPdIII & \aDepPdIV & \aDepPdV & \aDepPdVI & \aDepPdVII & \aDepPdVIII \tabularnewline
pnc & \aDepPnc & \aDepPncAll & \aDepPncI & \aDepPncII & \aDepPncIII & \aDepPncIV & \aDepPncV & \aDepPncVI & \aDepPncVII & \aDepPncVIII \tabularnewline
ROOT & root of sentence & \aDepRootAll & \aDepRootI & \aDepRootII & \aDepRootIII & \aDepRootIV & \aDepRootV & \aDepRootVI & \aDepRootVII & \aDepRootVIII \tabularnewline
sb & \aDepSb & \aDepSbAll & \aDepSbI & \aDepSbII & \aDepSbIII & \aDepSbIV & \aDepSbV & \aDepSbVI & \aDepSbVII & \aDepSbVIII \tabularnewline
svp & separable verb prefix & \aDepSvpAll & \aDepSvpI & \aDepSvpII & \aDepSvpIII & \aDepSvpIV & \aDepSvpV & \aDepSvpVI & \aDepSvpVII & \aDepSvpVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Lemmatization (\texttt{lemma})} The base form (root) of a word.


\subsubsection*{Common Word (\texttt{stop})}
This column's cell provides information if the word is part of a stop list, hence one of the most common words in the German language or not (\texttt{True} vs. \texttt{False}).


\subsubsection*{Word embedding (\texttt{vector})}
% \href{https://en.wikipedia.org/wiki/Word2vec}{word vector}
A 300-dimensional word vector providing a multi-dimensional meaning
representation of a word.
% handling of words unknown to the model
Out-of-vocabulary words with a vector consisting of 300 dimensions of zeroes
were set to \texttt{\#} to save space.


\subsection*{Dataset content}
% \href{https://bids.neuroimaging.io/}{bids.neuroimaging.io} TextGrid format
The annotation comes in two different versions. First, as a text-based TextGrid
file (\texttt{annotation/fg\_rscut\_ad\_ger\_speech\_tagged.TextGrid}) to be
conveniently edited using the software Praat \citep{boersma2019praat}.
% TSV format
Second, as a text-based, tab-separated-value (TSV) formatted table
(\texttt{annotation/fg\_rscut\_ad\_ger\_speech\_tagged.tsv}) in accordance with
the brain imaging data structure (\href{https://bids.neuroimaging.io/}{BIDS})
\citep{gorgolewski2016bids}.
% source code for descriptive statistics
The source code for all descriptive statistics included in this paper is
available in \texttt{code/descriptive-statistics.py} (Python script).


\section*{Dataset validation}
% Information about any validation carried out and/or any limitations of the
% datasets, including any allowances made for controlling bias or unwanted
% sources of variability. ~170 words => das wird beim folgenden Schreiben
% definitiv nichts werden
% intro
In order to assess data quality, we investigated if contrasting speech-related
events to events without speech lead to increased activation in areas known to
be involved in semantic processing \citep{hickok2007cortical}.
% nouns and proper nouns resp. > coordinate junctions
Moreover, we tested if similar linguistic concepts providing high semantic
information (proper nouns and nouns) contrasted to a concept providing low
semantic information (coordinate conjunctions) lead to increased activation in
congruent brain areas.
% data source
% data aligned by applying an affine transformation only are available in
% bold_dico_dico7Tad2grpbold7Tad.nii.gz.
% data aligned by non-linear warping are available in
% bold_dico_dico7Tad2grpbold7Tad_nl.nii.gz.
We used a dataset providing blood oxygenation level-dependent (BOLD) functional
magnetic resonance imaging (fMRI) data of 20 subjects (age 21–38 years, mean age
26.6 years, 12 male) listening to the 2h audio-description (\unit[7]{Tesla},
\unit[2]{s} repetition time, 3599 volumes, 36 axial slices, thickness
\unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane resolution, \unit[224]{mm}
field-of-view) \citep{hanke2014audiomovie}.
% motion correction
Data were already corrected for motion at the scanner computer.
% alignment
Further, individual BOLD time-series were already aligned by non-linear warping
to a study-specific T2*-weighted echo planar imaging (EPI) group template (cf.
\citep{hanke2014audiomovie} for details on how the group template was created).
% steps done by C.H.
All further analysis steps were carried out using Feat v6.00 (FMRI Expert
Analysis Tool)\citep{woolrich2001autocorr} as part of FSL v5.0.9 (FMRIB's
Software Library)\citep{smith2004fsl}.
% dropping of subject 10
Data of one participant were dropped to due to invalid distortion correction
during scanning.
% temporal and spatial filter, brain extraction
Data were temporally high-pass filtered (cut-off \unit[150]{s}), spatially
smoothed (Gaussian kernel; \unit[4.0]{mm} FWHM), and the brain was extracted
from surrounding tissue.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.


\begin{table*}[t]
    \caption{Overview of events that were used to create the 26 regressors of the GLM analysis.
The respective counts are given for the whole stimulus and the eight segments
that were used during fMRI scanning.
The 20 most often occurring labels from the detailed part-of speech tagging     (\texttt{tag}) were used as such.
Words belonging to all other labels were pooled to \texttt{tag\_other}.
The label \texttt{sentence} contains the end of complete grammatical sentences.
The label \texttt{phones} contains events of the 80 most often occurring
phonemes (phoneme \texttt{n} with N=6053 to phoneme \texttt{IY1} with N=32).
The label \texttt{no-sp} represents moments when no speech was audible. \texttt{fg\_ad\_lrdiff} (left-right volume difference) and \texttt{fg\_ad\_rms} (root mean square volume) represent one event for every movie frame (\unit[40]{ms}).
\todo[inline]{where from, or how created?}
Events were convolved with FSL's ``Double-Gamma HRF'' to create the regressors.
The correlation of these regressors over the time course of the whole stimulus
can be seen in Figure \ref{fig:reg-corr}.}
\label{tab:regressors}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
adja & \aTagAdja & \rAdjaAll & \rAdjaI & \rAdjaII & \rAdjaIII & \rAdjaIV & \rAdjaV & \rAdjaVI & \rAdjaVII & \rAdjaVIII \tabularnewline
adjd & \aTagAdjd & \rAdjdAll & \rAdjdI & \rAdjdII & \rAdjdIII & \rAdjdIV & \rAdjdV & \rAdjdVI & \rAdjdVII & \rAdjdVIII \tabularnewline
adv & \aTagAdv & \rAdvAll & \rAdvI & \rAdvII & \rAdvIII & \rAdvIV & \rAdvV & \rAdvVI & \rAdvVII & \rAdvVIII \tabularnewline
appr & \aTagAppr & \rApprAll & \rApprI & \rApprII & \rApprIII & \rApprIV & \rApprV & \rApprVI & \rApprVII & \rApprVIII \tabularnewline
apprart & preposition with article & \rApprartAll & \rApprartI & \rApprartII & \rApprartIII & \rApprartIV & \rApprartV & \rApprartVI & \rApprartVII & \rApprartVIII \tabularnewline
art & \aTagArt & \rArtAll & \rArtI & \rArtII & \rArtIII & \rArtIV & \rArtV & \rArtVI & \rArtVII & \rArtVIII \tabularnewline
kon & \aTagKon & \rKonAll & \rKonI & \rKonII & \rKonIII & \rKonIV & \rKonV & \rKonVI & \rKonVII & \rKonVIII \tabularnewline
ne & \aTagNe & \rNeAll & \rNeI & \rNeII & \rNeIII & \rNeIV & \rNeV & \rNeVI & \rNeVII & \rNeVIII \tabularnewline
nn & \aTagNn & \rNnAll & \rNnI & \rNnII & \rNnIII & \rNnIV & \rNnV & \rNnVI & \rNnVII & \rNnVIII \tabularnewline
pds & substituting demonstrative pronoun & \rPdsAll & \rPdsI & \rPdsII & \rPdsIII & \rPdsIV & \rPdsV & \rPdsVI & \rPdsVII & \rPdsVIII \tabularnewline
pis & substituting indefinite pronoun & \rPisAll & \rPisI & \rPisII & \rPisIII & \rPisIV & \rPisV & \rPisVI & \rPisVII & \rPisVIII \tabularnewline
pper & \aTagPper & \rPperAll & \rPperI & \rPperII & \rPperIII & \rPperIV & \rPperV & \rPperVI & \rPperVII & \rPperVIII \tabularnewline
pposat & \aTagPposat & \rPposatAll & \rPposatI & \rPposatII & \rPposatIII & \rPposatIV & \rPposatV & \rPposatVI & \rPposatVII & \rPposatVIII \tabularnewline
prf & reflexive personal pronoun & \rPrfAll & \rPrfI & \rPrfII & \rPrfIII & \rPrfIV & \rPrfV & \rPrfVI & \rPrfVII & \rPrfVIII \tabularnewline
ptkvz & \aTagPtkvz & \rPtkvzAll & \rPtkvzI & \rPtkvzII & \rPtkvzIII & \rPtkvzIV & \rPtkvzV & \rPtkvzVI & \rPtkvzVII & \rPtkvzVIII \tabularnewline
vafin & \aTagVafin & \rVafinAll & \rVafinI & \rVafinII & \rVafinIII & \rVafinIV & \rVafinV & \rVafinVI & \rVafinVII & \rVafinVIII \tabularnewline
vmfin & finite verb, modal & \rVmfinAll & \rVmfinI & \rVmfinII & \rVmfinIII & \rVmfinIV & \rVmfinV & \rVmfinVI & \rVmfinVII & \rVmfinVIII \tabularnewline
vvfin & \aTagVvfin & \rVvfinAll & \rVvfinI & \rVvfinII & \rVvfinIII & \rVvfinIV & \rVvfinV & \rVvfinVI & \rVvfinVII & \rVvfinVIII \tabularnewline
vvinf & \aTagVvinf & \rVvinfAll & \rVvinfI & \rVvinfII & \rVvinfIII & \rVvinfIV & \rVvinfV & \rVvinfVI & \rVvinfVII & \rVvinfVIII \tabularnewline
vvpp & \aTagVvpp & \rVvppAll & \rVvppI & \rVvppII & \rVvppIII & \rVvppIV & \rVvppV & \rVvppVI & \rVvppVII & \rVvppVIII \tabularnewline
tag\_other & all other TAG categories & \rTagotherAll & \rTagotherI & \rTagotherII & \rTagotherIII & \rTagotherIV & \rTagotherV & \rTagotherVI & \rTagotherVII & \rTagotherVIII \tabularnewline
%\hline
sentence & complete grammatical sentences & \rSentenceAll & \rSentenceI & \rSentenceII & \rSentenceIII & \rSentenceIV & \rSentenceV & \rSentenceVI & \rSentenceVII & \rSentenceVIII \tabularnewline
phones & 80 most often occurring phonemes & \rPhonesAll & \rPhonesI & \rPhonesII & \rPhonesIII & \rPhonesIV & \rPhonesV & \rPhonesVI & \rPhonesVII & \rPhonesVIII \tabularnewline
no-sp & no-speech (``soundscape'') & \rNospAll & \rNospI & \rNospII & \rNospIII & \rNospIV & \rNospV & \rNospVI & \rNospVII & \rNospVIII \tabularnewline
%\hline
fg\_ad\_lrdiff & left-right volume difference & \rFgadlrdiffAll & \rFgadlrdiffI & \rFgadlrdiffII & \rFgadlrdiffIII & \rFgadlrdiffIV & \rFgadlrdiffV & \rFgadlrdiffVI & \rFgadlrdiffVII & \rFgadlrdiffVIII \tabularnewline
fg\_ad\_rms & root mean square & \rFgadrmsAll & \rFgadrmsI & \rFgadrmsII & \rFgadrmsIII & \rFgadrmsIV & \rFgadrmsV & \rFgadrmsVI & \rFgadrmsVII & \rFgadrmsVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


% intro to analysis
We implemented a standard three-level, voxel-wise general linear model (GLM) to
average parameter estimates across the eight stimulus segments, and later across
19 subjects.
% 1st lvl
At the first level analyzing each segment for each subject individually, we
created 26 regressors based on events drawn from the annotation (see Table
\ref{tab:regressors}).
% part-of-speech labels: labels to events, top 20
The 20 most often occurring detailed part-of-speech labels (\texttt{nn} with
N=\rNnAll\space to text{prf} with N=\rPrfAll) were modeled as boxcar function
from onset to offset of each word.
% path-of-speech labels: remaining labels
The remaining other part-of-speech labels were pooled to a single new label
(\texttt{tag\_other}; N=\rTagotherAll) and each event was modeled as a boxcar
function from onset to offset of the corresponding word.
% phonemes
The 80 most often occurring phonemes (\texttt{n} with N=6053 to \texttt{IY1}
with N=32) were pooled to \texttt{phonemes} (N=\rPhonesAll) and each event
modeled as boxcar function from onset to offset of the corresponding phoneme.
% sentence (endings)
The end of each complete grammatical sentence was modeled as an impulse event
(N=\rSentenceAll) to capture variance correlating with sentence comprehension.
% no-speech intro
``No-speech'' events (\texttt{no-sp}; N=\rNospAll) serving as a control
condition were created by the following rationale: events were fitted into
longer lasting intervals between onsets and offsets of words and thus within
intervals without audible speech.
% minimum distance to words
Each event of the no-speech condition had to have a minimum distance of
\unit[1.8]{s} to an onsets or offsets of a word, and to each onset to another
event of the no-speech condition.
% length of no-speech events
A length of \unit[70]{ms} was chosen for no-speech events matching the average
length of phonemes.
% low-level nuisance regressors
Lastly, we used continuous bins of information about auditory features
(left-right difference in volume and root mean square energy) that was averaged
across the length of every movie frame (\unit[40]{ms}) to capture variance
correlating with low-level perceptual processes.
% convolving
Time series of events were convolved with FSL's ``Double-Gamma HRF'' as a model
of the hemodynamic response function to create the actual regressors.
% reference to figure ``correlation of regressors''
The Pearson correlation coefficients of the 26 regressors across the time course
of all stimulus segments can be seen in Figure \ref{fig:reg-corr}.
% temporal derivatives
Temporal derivatives were also included in the design matrix to compensate for
regional differences between modeled and actual HRF.
% motion parameters & temporal filtering of design motion parameter are
% filtered, too. See FSL manual: ``All (confound) EVs will be filtered to match
% the processing applied to the input data''
Finally, six motion parameters were used as additional nuisance regressors and
the design was subjected to the same temporal filtering as the BOLD time series.

% t-contrasts
The following three t-contrasts were defined:
1) words (all 21 \texttt{tag}-related regressors) > no-speech (\texttt{no-sp}),
% 1b) no-speech > words
2) proper nouns (\texttt{ne}) > coordinate conjunctions (\texttt{kon}; N=\rKonAll), and
% 2b) coordinate conjunctions > proper nouns
3) nouns (\texttt{nn}) > coordinate conjunctions.
% 3b) coordinate conjunctions > nouns

\begin{figure*} \centering
\includegraphics[width=\linewidth]{figures/regressor-corr}
\caption{Pearson correlation coefficients of the 26 regressors used in the
    analysis to validate the annotation.
    Regressors were convolved with FSL's ``Double-Gamma HRF'' as a model of the
    hemodynamic response function and concatenated across runs before
    performing the correlation.}
\label{fig:reg-corr} \end{figure*}


% 2nd lvl
The second-level analysis which averaged contrast estimates across the eight
stimulus segments per subject was carried out using a fixed effects model by
forcing the random effects variance to zero in FLAME (FMRIB's Local Analysis of
Mixed Effects) \citep{beckmann2003general, woolrich2004multilevel}.
% 3rd lvl
The third level analysis which averaged contrast estimates across subjects was
carried out using FLAME stage 1 with automatic outlier deweighting
\citep{woolrich2004multilevel, woolrich2008robust}.
% thresholding
Z (Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>3.4 and a corrected cluster significance threshold of p<.05
\citep{woolrich2008robust}.
% identification of brain regions
Brain regions associated with observed clusters were determined with the Jülich
Histological Atlas \citep{eickhoff2005toolbox,eickhoff2007assignment} and the
Harvard-Oxford Cortical Atlas \citep{desikan2006automated} provided by FSL.


% Results
Figure \ref{fig:results} depicts the results of the three contrasts (Z threshold
Z>3.4; p<.05 cluster-corrected).
% COPE 1:
The contrast words > no-speech yielded four significant clusters (see Table
\ref{tab:cope1}):
% cluster 1
one left lateralized cluster spanning from the angular gyrus and inferior
posterior supramarginal gyrus across the superior and middle temporal gyrus,
including parts of Heschl's gyrus and planum temporale.
% cluster 2
A second left cluster in (inferior) frontal regions, including precentral gyrus,
pars opercularis (Brodmann Areal 44; BA44) and pars triangularis (BA45).
% cluster 3
Similarly in the right hemisphere, one cluster spanning from the angular gyrus
across the superior and middle temporal gyrus but including frontal inferior
regions (pars opercularis and pars triangularis).
% cluster 4
A fourth significant cluster is located in the left thalamus.

% COPE 3
The contrast proper nouns > coordinate conjunctions yielded nine significant
clusters (see Table \ref{tab:cope3}):
% cluster 1
one left lateralized cluster spanning from the angular gyrus across planum
temporale and superior temporal gyrus, partially covering the Heschl's gyrus,
into the anterior middle temporal gyrus.
% cluster 2
A largely congruent but smaller cluster in the right hemisphere.
% cluster 3 & 5, and 4 & 6
Two clusters in posterior cingulate cortex and precuneus of both hemispheres.
% remaining 3 clusters of COPE 3
Three small clusters in the right occipital pole, right Heschl's gyrus and left
superior lateral occipital pole.

% COPE 5
The contrast nouns > coordinate conjunctions yielded four significant clusters (see Table \ref{tab:cope5}):
% cluster 1 & 2
two clusters that are slightly smaller than lateral temporal clusters of contrast nouns > coordinate conjunction.
% variation in cluster 1 & 2 in comparison to COPE 1
In this case, spanning from angular gyrus in the left hemisphere and from planum temporale in the right hemisphere into the anterior part of superior temporal cortex.
% cluster 3 & 4
Finally, two small right lateralized clusters in the right posterior cingulate
gyrus and right precuneus.


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/slicescolorbars}
    \caption{Mixed-effects group-level (N=14) GLM contrasts for the
        audio-description of the movie Forrest Gump. Significant clusters
        (Z>3.4, p<0.05 cluster-corrected) are overlaid on the custom T2* EPI
        group template (gray; taken from \citep{hanke2014audiomovie}) which is
    aligned to and overlaid on the MNI152 T1-weighted head template (copper).}
    \label{fig:results} \end{figure*}


\begin{table*}[t]
\caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected) for the contrast words (all 21 \texttt{tag}-related regressors) > no-speech.
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope1}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
14990 & <.001 & 6.31 & -49 & -24.7 & 6.35 & -54.8 & -32.5 & 3.73 &
left Heschl's g.;
lateral superior occipital c., angular g., superior and middle temporal g. (posterior to anterior);
parts of supramarginal g. \&, planum temporale \\
14469 & <.001 & 6.48 & 55 & -14.9 & -6.9 & 54.1 & -23.1 & 0.374 &
right superior temporal g.;
angular g., superior (and middle) temporal g. (posterior to anterior), Heschl's g.;
parts of supramarginal g., planum temporale, pars opercularis (BA44) \& pars triangularis (BA45) \\
1971 & <.001 & 5.26 & -51.1 & 25.6 & -10.5 & -53.6 & 17.8 & 10.2 & left frontal orbital c.;
pars opercularis (BA44), pars triangularis (BA45);
parts of precentral g. \\
217 & .002 & 4.55 & -4.48 & -13.7 & 10.3 & -6.46 & -14.9 & 9.96 & left Thalamus \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected) for the contrast proper nouns (\texttt{ne}) > coordinate conjunctions (\texttt{kon}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope3}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
7691 & <.001 & 6.23 & -61.2 & -22.3 & 11.6 & -55.9 & -20.7 & 4.03 &
left lPanum temporale; posterior inferior supramarginal g., superior temporal g., planum polare,
parts of posterior angular g.,  Heschl's g., middle temporal gyrus \\
5928 & <.001 & 5.5 & 57.5 & -26.2 & 15.9 & 58.2 & -15.8 & 3.55 &
right planum temporale;
Heschl's g., superior temporal g., planum polare, temporal pole;
parts of angular g. \& posterior inferior supramarginal gyrus \\
479 & <.001 & 4.62 & -5.42 & -32.3 & 25.3 & -4.28 & -39.4 & 22.8 & left posterior cingulate g. \\
420 & <.001 & 4.85 & -4.76 & -71.4 & 40.1 & -3.74 & -68.5 & 36.2 & left precuneus \\
407 & <.001 & 5.07 & 6.83 & -40.1 & 24.5 & 6.67 & -38.7 & 23.1 & right posterior cingulate c. \\
294 & <.001 & 4.57 & 17 & -69.1 & 34.6 & 17.7 & -67.1 & 34.9 & right precuneus \\
121 & .024 & 3.95 & 8.12 & -98.2 & 0.359 & 8.75 & -97.7 & -3.15 & right occipital pole \\
117 & .027 & 4.38 & 36.9 & -24.8 & 4.55 & 37.4 & -23 & 3.09 & right Heschl's g. \\
115 & .029 & 4.08 & -44.6 & -71.7 & 21.7 & -43.6 & -70.8 & 23.4 & left superior lateral occipital c.\\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\caption{Significant clusters (Z-Threshold Z>3.4; p<.05 cluster-corrected) for the contrast nouns (\texttt{nn}) > coordinate conjunctions (\texttt{kon}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope5}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
3166 & <.001 & 5.75 & -61.3 & -10.6 & -2.93 & -57.7 & -14.3 & 1.47 &
left anterior superior (and middle) temporal g.;
planum temporale, planum polare, anterior superior temporal g.;
part of posterior supramarginal g., Heschl's g. \\
1753 & <.001 & 4.99 & 63.3 & -15.1 & 8.41 & 58 & -13 & 4.02 & right planum temporale, anterior superior temporal g., planum polare;
part of\& part of Heschl's G. \\
166 & .004 & 4.5 & 6.83 & -40.1 & 24.5 & 7.01 & -39.7 & 24.2 &
right posterior cingulate g. \\
149 & .008 & 4.13 & 18.2 & -67.8 & 36 & 19.8 & -66.4 & 34.6 &
right precuneus \\
\bottomrule
\end{tabular}
\end{table*}


% Discussion words > no-speech
For the contrast words > no-speech, results show increased hemodynamic activity
in a bilateral cortical network including temporal, parietal and frontal regions
related to processing spoken language \citep{friederici2011brain,
hickok2007cortical,price2012twentyyears}.
% similarities to previous naturalistic speech paradigms
These clusters resemble results of previous studies that implemented an ISC
approach to analyze fMRI data of naturalistic auditory stimuli
\citep{honey2012not, lerner2011topographic, silbert2014coupled}.
% differences
We don't find significantly increased activations in midline areas (like the
posterior cingulate cortex and precuneus or anterior cingulate cortex and medial
frontal cortex) which showed correlated activity across subjects in previous
studies.
% Wilson (2008): ISC + GLM
In this regard, our results are similar to \citep{wilson2008beyond} who
implemented both an ISC and a GLM analysis. The findings by
\citep{wilson2008beyond} were that an ISC analysis yielded correlated activity
in midline areas. But the GLM analysis contrasting blocks of listening to
narratives to blocks of a resting condition yielded decreased activity in
midline areas.
\todo{fun fact regarding reverse contrast (no-speech > words):
    clusters in posterior \& anterior cingulate (and subcallosal c. \& insulae
bilaterally); could be mentioned, must be mentioned? How shortly? No figure,
pls!}

% (proper) nouns > con
The two contrasts contrasting nouns and proper nouns respectively to coordinate
junctions yielded increased activation partially located in early sensory
regions (Heschl's Gyrus; \citep{saenz2014tonotopic}) and most prominently
adjacent regions bilaterally (planum temporale; superior temporal gyrus;
\citep{arsenault2015distributed, mesgarani2014phonetic}).
% why these contrasts nous & proper nouns
We chose nouns and proper nouns for these two contrasts because they represent
linguistically similar concepts but are uncorrelated in the German language and
stimulus (cf. \ref{fig:reg-corr})
% coordinate conjunctions
We contrasted nouns and proper nouns respectively to coordinate conjunctions
because (proper) nouns and coordinate conjunctions are both linguistically
different and uncorrelated.
% “interpretation”
\todo{well what's combing up next is adventurous} Despite the fact that nouns
and proper nouns are uncorrelated, contrasts lead to spatially similar clusters
``which were not driven by a negative correlation of coordinate conjunctions
with the modeled time series (s. totally private data)''.
% why it's cool
Results suggest that our current annotation of naturalistic stimuli can be used
to create model-based analyses to test neuroscientific hypotheses.

\todo[inline]{following stuff is still here but can probably be deleted}
% interpretation 1a: large amount of semantically different nouns > narrow
% concept (conjunctions)
This could be attributed to the exploratory nature of our analysis. A high count
of heterogeneous nouns (N=\rNnAll) and proper nouns (N=\rNeAll) that were
averaged across all 300 semantic dimensions (cf. \citep{mitchell2008predicting,
huth2016natural}), and contrasted to coordinate conjunctions that offered a
comparably small amount of events (N=\rKonAll).
% interpretation 1b: not just narrow concept but also dominated by just two
% different words
Coordinate junctions were chosen because they were assumed to be temporally
uncorrelated with nouns but mostly consist of the words ``and'' (N=392) and
``but'' (N=49), and thus offered only a small amount of variance.
% interpretation 2
A second explanation might be that our phoneme regressors that assumed linear
summation of approximately \unit[70]{ms} lasting events in quick succession did
not capture a sufficiently amount of nuisance variance


\section*{Data availability}
% aus der Schnitt-Annotation übernommen
\texttt{This section will be auto-generated.}

In addition, released data, code, and manuscript sources are also available on
Github (\url{https://github.com/psychoinformatics-de/studyforrest-paper-speechannotation}).

\todo[inline]{the paper needs to serve a purpose:
    a) documentation for the annotation.
    b) attract people to use this work for their own studies.
    I would even argue that this is the \textit{main} purpose, given that the
    annotation in itself represents to scientific progress. The need to give an
    accurate assessment what this can be used for and what is problematic or
    impossible.  ATM this is the only statement in this regard: 'facilitates
    modeling of hemodynamic brain responses correlating with speech-related
events' and the only indication of purpose is 'additional confound measures'. I
thin this is incongruent with the amount of work and the level of detail of
this annotation; CH: let's discuss that personally}

\section*{Conclusions}
% what the paper is about
As an extension of the studyforrest dataset, we present an annotation of spoken
language in the two hours lasting ``research cut'' of the audio-visual movie
``Forrest Gump'' and its temporally aligned audio-description.
% what we did
We annotated the onset and offset of sentences, words and phonemes, and the
corresponding speaker's identity.
% what for
An additional tagging of each word's linguistic features enable independent
working groups to create sophisticated models of hemodynamic activity
correlating with linguistics aspects like phonology, grammar, syntax, and
semantics occurring in two naturalistic stimuli.
% results
Results of our dataset validating analysis that we run as a proof of concept
show that the annotation's content can indeed / in principle be used to
isolate/track speech-related networks in the human brain under life-like
conditions.


\subsection*{Author contributions}
% subsection in Schnitte-Anno
%In order to give appropriate credit to each author of an article, the
%individual contributions of each author to the manuscript should be detailed
%in this section. We recommend using author initials and then stating briefly
%how they contributed.
COH designed, performed, and validated the annotation, and wrote the manuscript.
MH provided critical feedback on the procedure and wrote the manuscript.

\subsection*{Competing interests}
% subsection in Schnitte-Anno
No competing interests were disclosed.

\subsection*{Grant information}
COH was supported by a graduate stipend from the German federal state of
Saxony-Anhalt and MH was supported by funds from the German federal state of
Saxony-Anhalt and the European Regional Development Fund (ERDF), Project:
Center for Behavioral Brain Sciences (CBBS). Work on the adapting data
management technology for this study was in part supported by the European
Union's Horizon 2020 Research and Innovation Programme under Grant Agreement
no. 785907 (HBP SGA2).


\subsection*{Acknowledgements}
COH is grateful to Valeri Kippes who took care of the author's mental sanity by providing excellent training at his gym in Jülich during the mentally draining period of manual corrections of the annotation.

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

\end{document}
