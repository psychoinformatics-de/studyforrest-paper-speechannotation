% For more detailed article preparation guidelines, please see:
% http://f1000research.com/author-guidelines
\documentclass[10pt,a4paper,onecolumn]{article}
\usepackage{f1000_styles}
\usepackage{units}
\geometry{right=4cm}
% enable the 'endfloat' package to sort all tables and figures to the
% end of the document without having to edit the manuscript
%\usepackage{endfloat}
\usepackage{booktabs}
\usepackage[
	colorlinks=true,
	urlcolor=blue,
	linkcolor=green
]{hyperref}
%% Default: numerical citations
\usepackage[numbers]{natbib}

%% Uncomment this lines for superscript citations instead
% \usepackage[super]{natbib}

%% Uncomment these lines for author-year citations instead
% \usepackage[round]{natbib}
% \let\cite\citep

\begin{document}
\input{descr-stats-anno.tex}
\input{descr-stats-regressors.tex}

\title{A studyforrest extension, an annotation of spoken language in the German dubbed movie ``Forrest Gump'' and its audio-description}
%\titlenote{whatever titlenote here}

\author[1, 2]{Christian~O.~Häusler}
\author[1, 2]{Michael~Hanke}

\affil[1]{Institute of Neuroscience and Medicine, Brain \& Behaviour (INM-7), Research Centre Jülich, Jülich, 52425, Germany}
\affil[2]{Institute of Systems Neuroscience, Medical Faculty, Heinrich Heine University, Düsseldorf, 40225, Germany}

\maketitle

\thispagestyle{fancy}

\begin{abstract}
% up to 300 words intro
Here we present an annotation of speech in the audio-visual movie
``Forrest Gump'' and its audio-description for a visually impaired audience,
as an addition to a large public functional brain imaging dataset
(\href{www.studyforrest.org}{studyforrest.org}).
% anno content 1
The annotation provides information about the exact timing of each of the more
than 2500 spoken sentences, 16000 words (including 202 non-speech
vocalizations), 66000 phonemes, and their corresponding speaker.
% anno content 2
Additionally, for every word, we provide lemmatization, a simple
part-of-speech-tagging (15 grammatical categories), a detailed part-of-speech
tagging (43 grammatical categories), syntactic dependencies, and a semantic
analysis based on word embedding which represents each word in a
300-dimensional semantic space.
% validation
To validate the dataset's quality, we build a model of hemodynamic brain
activity based on information drawn from the annotation.
% why bother
Results suggest that the annotation's content and quality enable independent
researchers to create models of brain activity correlating with a
variety of linguistic aspects under conditions of near-real-life complexity.
\end{abstract}


\section*{Contacts}
\begin{description}
  \item{Christian~O.~Häusler} \\
    email: \href{mailto:c.haeusler@fz-juelich.de}{c.haeusler@fz-juelich.de} \\
    orcid: https://orcid.org/0000-0002-0936-317X
  \item{Michael~Hanke}\\
    email: \href{mailto:michael.hanke@gmail.com}{michael.hanke@gmail.com} \\
    orcid: https://orcid.org/0000-0001-6398-6370 \\
\end{description}

\section*{Keywords}
% maximal 8
annotation, language, speech, narrative, naturalistic stimulus, fMRI, studyforrest

\clearpage


\section*{Introduction}
% intro to intro
Cognitive and psychiatric neuroimaging are moving towards studying brain
functions under conditions of lifelike complexity
\citep{sonkusare2019naturalistic, eickhoff2020towards}. Motion pictures
\citep{hasson2008neurocinematics} and continuous narratives
\citep{wilson2008beyond, lerner2011topographic} are increasingly utilized as so called ``naturalistic
stimuli''.
% not designed for research
Naturalistic stimuli are usually designed for commercial purposes and to
entertain their audiences.
% thus annotation bottleneck
Thus, the temporal structure of their feature space is usually not explicitly
known, leading to an ``annotation bottleneck'' \citep{aliko2020naturalistic} when
used for neuroscientific research.

% data-driven methods..
Data-driven methods like inter-subject correlation
(ISC)\citep{hasson2004intersubject} or independent component analysis
(ICA)\citep{bartels2004chronoarchitecture} are often used to analyze
such fMRI data in order to circumvent this bottleneck.
% ..are nice but:
However, use of data-driven methods alone falls short of associating results with
particular stimulus events \citep{kauttonen2015optimizing}.
% model-driven methods
Model-driven methods, like the general linear model (GLM), that are based on
stimulus annotations can be useful to test hypotheses on specific brain
functions under more ecologically valid conditions, to statistically control
confounding stimulus features, and to explain not just ``how'' the brain is
responding to a stimulus but also ``why'' \citep{hamilton2018revolution}.
% GLM: example studies
Studies using GLMs based on annotations of a stimulus' temporal structure have
elucidated, for example, how the brain responds to visual features of a movie
\citep{bartels2004mapping} or speech-related features of a narrative
\citep{rocca2020language}.
% combination of data-driven and model-driven approaches
Furthermore, stimulus annotations can inform data-driven methods about a
stimulus' temporal dynamics, or model-driven and data-driven methods can be
combined to improve the interpretability of results \citep{lahnakoski2012stimulus}.

% This publication
% there was stuff that needed to be honed
%For this publication, we revised previous, preliminary annotations done by our
%lab.
% now the honed stuff for AV and AO
Here we provide an annotation with exact onset and offset of each
sentence, word and phoneme (see Table \ref{tab:overview} for an overview) spoken
in the audio-visual movie ``Forrest Gump'' \citep{ForrestGumpMovie} and its
audio-description (i.e. the movie's soundtrack with an additional
narrator)\citep{ForrestGumpGermanAD}.
% the studyforrest dataset
fMRI data of participants watching the audio-visual movie
\citep{hanke2016simultaneous} and listening to the audio-description
\citep{hanke2014audiomovie} are the core data of the publicly available
\textit{studyforrest} dataset (\href{www.studyforrest.org}{studyforrest.org}).
% why even bother?
The current publication enables researchers to model hemodynamic brain responses
that correlate with a variety of aspects of spoken language ranging from a
speaker's identity, to phonetics, grammar, syntax, and semantics.
% extending previous non-speech annotations
This publication extends already available annotations of portrayed emotions
\citep{labs2015portrayed}, perceived emotions \citep{lettieri2019emotionotopy},
as well as cuts and locations depicted in the movie
\citep{haeusler2016annotation}.
% just use it all
All annotations can be used in any study focusing on aspects of real-life
cognition by serving as additional confound measures describing the  temporal
structure and feature space of the stimuli.


\section*{Materials and methods}

\subsection*{Stimulus}
% stimulus != DVD but ``research cut''
We annotated speech in the slightly shortened ``research cut''
\citep{hanke2014audiomovie} of the movie ``Forrest Gump'' and its temporally
aligned audio-description \citep{hanke2016simultaneous} that was broadcast as
an additional audio track for visually impaired listeners on Swiss public
television \citep{ForrestGumpGermanAD}.
%  what the AO contains
The plot of the original movie is already carried by an off-screen voice of the
main character Forrest Gump. In the audio-description, an additional male
narrator describes essential aspects of the visual scenery when
there is no off-screen voice, dialog, or other relevant auditory content.

\subsection*{Annotation procedure}
% preliminary annotations
Preliminary, manual orthographic transcripts of dialogues, non-speech
vocalizations (e.g. laughter or groaning) and the script for the
audio-description's narrator were merged and converted to Praat's
\citep{boersma2019praat} TextGrid format.
% preparing annotation in Praat before forced alignment
% \href{www.fon.hum.uva.nl/praat}{fon.hum.uva.nl/praat}.
This merged transcript contained a rough onset and offset timings for small
groups of sentences, and was further edited in Praat for manual validation
against the actual content of the audio material.
% steps conducted
The following steps were performed by a single person, already familiar with
the stimulus, in several passes to iteratively improve the
quality of the data: approximate temporal onsets and offsets were corrected;
intervals containing several sentences were split into intervals containing only
one sentence;
% dropping of sentences hard to understand
when two or more persons were speaking simultaneously the less dominant voice
was dropped; low volume non-speech vocalizations or low volume background speech
(especially during music or continuous environmental noise) which were
subjectively assessed to be incomprehensible for the audience were dropped, too.

% intro for forced aligner
We then used the Montreal Forced Aligner \citep{mcauliffe2017montreal} to
algorithmically identify the exact onset and offset of each word and phoneme.
% preparing other inputs before forced alignment pronunciation dictionary
% http://prosodylab.org
% https://raw.githubusercontent.com/prosodylab/prosodylab.dictionaries/master/de.dict
To enable the aligner to look up the phonemes embedded within each word, we
chose a German pronunciation dictionary provided by Prosodylab \citep
{gorman2011prosodylab} that uses the Prosodylab PhoneSet to describe the
pronunciation of phonemes.
% extending the dictionary with formerly unknown German words
To improve the detection rate of the automatic alignment, the dictionary was
manually updated with German words that occur in the stimuli but were originally
missing in the dictionary.
% \href{http://mlmlab.org/mfa/dictionaries/english.dict}{mlmlab.org/mfa/dictionaries/english.dict}
% English words in the otherwise German stimulus
The pronunciation of English words and phonemes occurring in the otherwise
German audio track was taken from an English pronunciation dictionary (following
the ARPAbet PhoneSet) provided by the Montreal Language Modeling lab
(\href{http://mlmlab.org/mfa/dictionaries/english.dict}{mlmlab.org}).
% performing the forced alignment audio input to the aligner
The audio track of the audio-description was converted from FLAC to WAV via
FFmpeg \citep{ffmpeg} to meet the aligner's input requirements.
% inputs
This WAV file, the merged transcription, and the updated dictionary were
submitted to the aligner that first trained an acoustic model on the data and
then performed the alignment.

% output
The resulting timings of words and
phonemes were corrected manually and iteratively in several passes using Praat:
% onset/offset checking & correction
In a first step, onsets and offsets on which the automatic alignment performed
moderately were corrected.
% handling poor performance
Some low volume sentences that are spoken in continuously noisy settings (e.g.
during battle or hurricane) were removed due to poor overall alignment
performance.
% adding whole sentences
In a second step, the complete sentences of the orthographic transcription were
copied into the annotation created by the aligner.
% speaker identity
In a third step, a speaker's identity was added for each sentence (see Table
\ref{tab:speakers} for the most often occurring speakers).
% constant on-the-fly checking of previous
During every step previous results were repeatedly checked for
errors and further improvements.

\begin{table*}[tbp] \caption{Overview of the annotation's content for the
        audio-description of ``Forrest Gump'' (i.e. the audio-only variant of
        the movie) that comprises the additional narrator. Counts are given for
        the whole stimulus (\texttt{all}) and its individual segments used
        during fMRI scanning.  The category \texttt{sentences} comprises
        complete grammatical sentences which are additionally marked in the
        annotation with a full stop at the end (``my feet hurt.'').  It also
        comprises questions (``do you want a chocolate?''), exclamations (``run
        away!''), or non-speech vocalizations in quick succession (``ha, ha,
        ha''), or in isolation (e.g. ``Forrest?'', ``Forrest!'', ``ha'') at time
        points when speakers switch rapidly. The category \texttt{words}
        comprises each word or non-speech vocalization (N=\aPosNonspeechAll) in
    isolation.} \label{tab:overview} \begin{tabular}{llllllllll} \toprule
        \textbf{category} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3}
        & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}\\
        \midrule sentences  & \aSentencesAll & \aSentencesI & \aSentencesII &
        \aSentencesIII & \aSentencesIV & \aSentencesV & \aSentencesVI &
        \aSentencesVII & \aSentencesVIII \tabularnewline words  & \aWordsAll &
        \aWordsI & \aWordsII & \aWordsIII & \aWordsIV & \aWordsV & \aWordsVI &
        \aWordsVII & \aWordsVIII \tabularnewline phonemes  & \aPhonesAll &
        \aPhonesI & \aPhonesII & \aPhonesIII & \aPhonesIV & \aPhonesV &
        \aPhonesVI & \aPhonesVII & \aPhonesVIII \tabularnewline \bottomrule
    \end{tabular} \end{table*}


\begin{table*}[btp]
\caption{Sentences spoken by the ten most often occurring speakers sorted alphabetically. The narrator only occurs in the audio-description.
Overall 97 persons were identified.
Names are mostly identical to the names used in \citep{labs2015portrayed}.}
\label{tab:speakers}
\begin{tabular}{llllllllll}
\toprule
\textbf{name} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8}\\
\midrule
Bubba  & \aBubbaAll & \aBubbaI & \aBubbaII & \aBubbaIII & \aBubbaIV & \aBubbaV & \aBubbaVI & \aBubbaVII & \aBubbaVIII \tabularnewline
Forrest  & \aForrestAll & \aForrestI & \aForrestII & \aForrestIII & \aForrestIV & \aForrestV & \aForrestVI & \aForrestVII & \aForrestVIII \tabularnewline
Forrest (child)  & \aForrestchildAll & \aForrestchildI & \aForrestchildII & \aForrestchildIII & \aForrestchildIV & \aForrestchildV & \aForrestchildVI & \aForrestchildVII & \aForrestchildVIII \tabularnewline
Forrest (v.o.)  & \aForrestvoAll & \aForrestvoI & \aForrestvoII & \aForrestvoIII & \aForrestvoIV & \aForrestvoV & \aForrestvoVI & \aForrestvoVII & \aForrestvoVIII \tabularnewline
Hancock  & \aHancockAll & \aHancockI & \aHancockII & \aHancockIII & \aHancockIV & \aHancockV & \aHancockVI & \aHancockVII & \aHancockVIII \tabularnewline
Jenny  & \aJennyAll & \aJennyI & \aJennyII & \aJennyIII & \aJennyIV & \aJennyV & \aJennyVI & \aJennyVII & \aJennyVIII \tabularnewline
Jenny (child)  & \aJennychildAll & \aJennychildI & \aJennychildII & \aJennychildIII & \aJennychildIV & \aJennychildV & \aJennychildVI & \aJennychildVII & \aJennychildVIII \tabularnewline
Lt. Dan  & \aLtdanAll & \aLtdanI & \aLtdanII & \aLtdanIII & \aLtdanIV & \aLtdanV & \aLtdanVI & \aLtdanVII & \aLtdanVIII \tabularnewline
Mrs. Gump  & \aMrsgumpAll & \aMrsgumpI & \aMrsgumpII & \aMrsgumpIII & \aMrsgumpIV & \aMrsgumpV & \aMrsgumpVI & \aMrsgumpVII & \aMrsgumpVIII \tabularnewline
Narrator  & \aNarratorAll & \aNarratorI & \aNarratorII & \aNarratorIII & \aNarratorIV & \aNarratorV & \aNarratorVI & \aNarratorVII & \aNarratorVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


% natural language processing (NLP); Tiger Treebank Corpus:
% \href{https://www.ims.uni-stuttgart.de}{ims.uni-stuttgart.de}.
We employed the Python package spaCy \citep{spacy2} and its accompanying German
language model (\texttt{de\_core\_news\_md}) that was trained on the TIGER
Treebank corpus \citep{brants2004tiger}) to automatically analyze linguistic
features of each word in their corresponding sentence.
% dropping of non-speech vocalizations because they often delude the algo
Non-speech vocalizations were dropped from the sentences before analysis to
improve results.
% performed analyses
We then performed analyses regarding part-of-speech (i.e. grammatical tagging or
word-category disambiguation), syntactic dependencies, lemmatization, word
embedding (i.e. a multi-dimensional meaning representation of a word), and if
the word is one of the most common words of the German language (i.e. if the
word is part of a stop list).


\subsection*{Data Legend}
% TextGrid vs. TSV
The annotation is available in two different versions, both providing the same
information: a) as a text-based Praat TextGrid file, and b) as a text-based,
tab-separated value (TSV) formatted table.
% preview of following subsubsections
The following descriptions refer to the ten columns of the TSV file, namely
\texttt{onset}, \texttt{duration}, \texttt{person}, \texttt{text}, \texttt{pos}, \texttt{tag}, \texttt{dep}, \texttt{lemma},
\texttt{stop}, \texttt{vector}.


\subsubsection*{Start (\texttt{start})} The onset of the sentence, word or
phoneme. Time stamps are provided in the format seconds.milliseconds from stimulus
onset.


\subsubsection*{Duration (\texttt{duration})} The duration of the sentence,
word or phoneme provided in the format seconds.milliseconds.


\subsubsection*{Speaker identity (\texttt{person})} Name of the person that
speaks the sentence, word or phoneme. See Table \ref{tab:speakers} for the ten
most often occurring speakers.


\subsubsection*{Text (\texttt{text})}
% ``Großphoneme'': n<200
The text of a spoken sentence or word, or the pronunciation of a phoneme.
Phonemes of German words follow the Prosodylab PhoneSet, English words follow
the ARPAbet PhoneSet.


\subsubsection*{Simple part-of-speech tag (\texttt{pos})} A simple
part-of-speech tagging (grammatical tagging; word-category disambiguation) of
words.
% annotation scheme
The tag labels of this simple part-of-speech tagging follow the Universal
Dependencies v2 POS tag set
(\href{https://universaldependencies.org}{universaldependencies.org}).
% reference to table
See Table \ref{tab:pos} for a description of the labels and the respective
counts of all 15 labels. Nouns that spaCy mistook for proper nouns or vice versa
were corrected via script.
% s. list at the beginning of script code/add_part-of-speech-tagging2textgrid.py
% flags for sentences and phonemes in POS column
Additionally in cells of this column, sentences are tagged as \texttt{SENTENCE},
and phonemes are tagged as \texttt{PHONEME} to facilitate filtering in potential
further processing steps.

\begin{table*}[t]
\caption{Simple part-of-speech tagging (\texttt{pos}) performed by the Python package spaCy \citep{spacy2}.
All 15 labels sorted alphabetically.
Descriptions were taken from spaCy.explain().
Non-speech vocalizations (\texttt{NONSPEECH}) were manually identified.
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:pos}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
ADJ & \aPosAdj & \aPosAdjAll & \aPosAdjI & \aPosAdjII & \aPosAdjIII & \aPosAdjIV
& \aPosAdjV & \aPosAdjVI & \aPosAdjVII & \aPosAdjVIII \tabularnewline
ADP & \aPosAdp & \aPosAdpAll & \aPosAdpI & \aPosAdpII & \aPosAdpIII & \aPosAdpIV & \aPosAdpV & \aPosAdpVI & \aPosAdpVII & \aPosAdpVIII \tabularnewline
ADV & \aPosAdv & \aPosAdvAll & \aPosAdvI & \aPosAdvII & \aPosAdvIII & \aPosAdvIV & \aPosAdvV & \aPosAdvVI & \aPosAdvVII & \aPosAdvVIII \tabularnewline
AUX & \aPosAux & \aPosAuxAll & \aPosAuxI & \aPosAuxII & \aPosAuxIII & \aPosAuxIV & \aPosAuxV & \aPosAuxVI & \aPosAuxVII & \aPosAuxVIII \tabularnewline
CONJ & \aPosConj & \aPosConjAll & \aPosConjI & \aPosConjII & \aPosConjIII & \aPosConjIV & \aPosConjV & \aPosConjVI & \aPosConjVII & \aPosConjVIII \tabularnewline
DET & \aPosDet & \aPosDetAll & \aPosDetI & \aPosDetII & \aPosDetIII & \aPosDetIV & \aPosDetV & \aPosDetVI & \aPosDetVII & \aPosDetVIII \tabularnewline
NONSPEECH & non-speech vocalization & \aPosNonspeechAll & \aPosNonspeechI & \aPosNonspeechII & \aPosNonspeechIII & \aPosNonspeechIV & \aPosNonspeechV & \aPosNonspeechVI & \aPosNonspeechVII & \aPosNonspeechVIII \tabularnewline
NOUN & \aPosNoun & \aPosNounAll & \aPosNounI & \aPosNounII & \aPosNounIII & \aPosNounIV & \aPosNounV & \aPosNounVI & \aPosNounVII & \aPosNounVIII \tabularnewline
NUM & \aPosNum & \aPosNumAll & \aPosNumI & \aPosNumII & \aPosNumIII & \aPosNumIV & \aPosNumV & \aPosNumVI & \aPosNumVII & \aPosNumVIII \tabularnewline
PART & \aPosPart & \aPosPartAll & \aPosPartI & \aPosPartII & \aPosPartIII & \aPosPartIV & \aPosPartV & \aPosPartVI & \aPosPartVII & \aPosPartVIII \tabularnewline
PRON & \aPosPron & \aPosPronAll & \aPosPronI & \aPosPronII & \aPosPronIII & \aPosPronIV & \aPosPronV & \aPosPronVI & \aPosPronVII & \aPosPronVIII \tabularnewline
PROPN & \aPosPropn & \aPosPropnAll & \aPosPropnI & \aPosPropnII & \aPosPropnIII & \aPosPropnIV & \aPosPropnV & \aPosPropnVI & \aPosPropnVII & \aPosPropnVIII \tabularnewline
SCONJ & \aPosSconj & \aPosSconjAll & \aPosSconjI & \aPosSconjII & \aPosSconjIII & \aPosSconjIV & \aPosSconjV & \aPosSconjVI & \aPosSconjVII & \aPosSconjVIII \tabularnewline
VERB & \aPosVerb & \aPosVerbAll & \aPosVerbI & \aPosVerbII & \aPosVerbIII & \aPosVerbIV & \aPosVerbV & \aPosVerbVI & \aPosVerbVII & \aPosVerbVIII \tabularnewline
X & \aPosX & \aPosXAll & \aPosXI & \aPosXII & \aPosXIII & \aPosXIV & \aPosXV & \aPosXVI & \aPosXVII & \aPosXVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Detailed part-of-speech tag (\texttt{tag})}
% \href{https://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/germantagsets}{Stuttgart-Tübingen-Tagset}
A detailed part-of-speech tagging of words following the TIGER Treebank
annotation scheme \citep{brants2004tiger} which is based on the
Stuttgart-Tübingen-Tagset \citep{schiller1999stts}.
% reference to table
See Table \ref{tab:tag} for a descripion  of the labels and the respective
counts of the 15 most often occuring labels (overall 43 labels). Nouns that
spaCy mistook for proper nouns or vice versa were corrected via script.
% s. list at the beginning of script code/add_part-of-speech-tagging2textgrid.py


\begin{table*}[t]
\caption{Detailed part-of-speech tagging (\texttt{tag}) performed by the Python package spaCy \citep{spacy2}.
The 15 most often occurring labels (overall 43 labels) sorted alphabetically.
Descriptions were taken from spaCy.explain().
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:tag}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
ADJA & \aTagAdja & \aTagAdjaAll & \aTagAdjaI & \aTagAdjaII & \aTagAdjaIII & \aTagAdjaIV & \aTagAdjaV & \aTagAdjaVI & \aTagAdjaVII & \aTagAdjaVIII \tabularnewline
ADJD & \aTagAdjd & \aTagAdjdAll & \aTagAdjdI & \aTagAdjdII & \aTagAdjdIII & \aTagAdjdIV & \aTagAdjdV & \aTagAdjdVI & \aTagAdjdVII & \aTagAdjdVIII \tabularnewline
ADV & \aTagAdv & \aTagAdvAll & \aTagAdvI & \aTagAdvII & \aTagAdvIII & \aTagAdvIV & \aTagAdvV & \aTagAdvVI & \aTagAdvVII & \aTagAdvVIII \tabularnewline
APPR & \aTagAppr & \aTagApprAll & \aTagApprI & \aTagApprII & \aTagApprIII & \aTagApprIV & \aTagApprV & \aTagApprVI & \aTagApprVII & \aTagApprVIII \tabularnewline
ART & \aTagArt & \aTagArtAll & \aTagArtI & \aTagArtII & \aTagArtIII & \aTagArtIV & \aTagArtV & \aTagArtVI & \aTagArtVII & \aTagArtVIII \tabularnewline
KON & \aTagKon & \aTagKonAll & \aTagKonI & \aTagKonII & \aTagKonIII & \aTagKonIV & \aTagKonV & \aTagKonVI & \aTagKonVII & \aTagKonVIII \tabularnewline
NE & \aTagNe & \aTagNeAll & \aTagNeI & \aTagNeII & \aTagNeIII & \aTagNeIV & \aTagNeV & \aTagNeVI & \aTagNeVII & \aTagNeVIII \tabularnewline
NN & \aTagNn & \aTagNnAll & \aTagNnI & \aTagNnII & \aTagNnIII & \aTagNnIV & \aTagNnV & \aTagNnVI & \aTagNnVII & \aTagNnVIII \tabularnewline
PPER & \aTagPper & \aTagPperAll & \aTagPperI & \aTagPperII & \aTagPperIII & \aTagPperIV & \aTagPperV & \aTagPperVI & \aTagPperVII & \aTagPperVIII \tabularnewline
PPOSAT & \aTagPposat & \aTagPposatAll & \aTagPposatI & \aTagPposatII & \aTagPposatIII & \aTagPposatIV & \aTagPposatV & \aTagPposatVI & \aTagPposatVII & \aTagPposatVIII \tabularnewline
PTKVZ & \aTagPtkvz & \aTagPtkvzAll & \aTagPtkvzI & \aTagPtkvzII & \aTagPtkvzIII & \aTagPtkvzIV & \aTagPtkvzV & \aTagPtkvzVI & \aTagPtkvzVII & \aTagPtkvzVIII \tabularnewline
VAFIN & \aTagVafin & \aTagVafinAll & \aTagVafinI & \aTagVafinII & \aTagVafinIII & \aTagVafinIV & \aTagVafinV & \aTagVafinVI & \aTagVafinVII & \aTagVafinVIII \tabularnewline
VVFIN & \aTagVvfin & \aTagVvfinAll & \aTagVvfinI & \aTagVvfinII & \aTagVvfinIII & \aTagVvfinIV & \aTagVvfinV & \aTagVvfinVI & \aTagVvfinVII & \aTagVvfinVIII \tabularnewline
VVINF & \aTagVvinf & \aTagVvinfAll & \aTagVvinfI & \aTagVvinfII & \aTagVvinfIII & \aTagVvinfIV & \aTagVvinfV & \aTagVvinfVI & \aTagVvinfVII & \aTagVvinfVIII \tabularnewline
VVPP & \aTagVvpp & \aTagVvppAll & \aTagVvppI & \aTagVvppII & \aTagVvppIII & \aTagVvppIV & \aTagVvppV & \aTagVvppVI & \aTagVvppVII & \aTagVvppVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Syntactic dependency (\texttt{dep})} Information about a word's
syntactic dependencies with other words within the same sentence.
% annotation scheme
Information follows the TIGER Treebank annotation scheme \citep{brants2004tiger}
and is given in the format: ``arc label;word's head;word's child1, word's
child2, ...'', where the ``arc label'' (see Table \ref{tab:dep}) describes the
type of syntactic relation that connects a "child" (the current word) to its
``head''.

\begin{table*}[t]
\caption{Syntactic dependencies (\texttt{dep}) performed by the Python package spaCy \citep{spacy2}.
The 15 most often occurring labels (overall 37 labels) sorted alphabetically.
Descriptions were taken from spaCy.explain().
Counts for the whole stimulus (\texttt{all}) and for each of the eight stimulus segments refer to the audio-description.}
\label{tab:dep}
\begin{tabular}{lllllllllll}
\toprule
\textbf{label} & \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
cd & \aDepCd & \aDepCdAll & \aDepCdI & \aDepCdII & \aDepCdIII & \aDepCdIV & \aDepCdV & \aDepCdVI & \aDepCdVII & \aDepCdVIII \tabularnewline
cj & \aDepCj & \aDepCjAll & \aDepCjI & \aDepCjII & \aDepCjIII & \aDepCjIV & \aDepCjV & \aDepCjVI & \aDepCjVII & \aDepCjVIII \tabularnewline
cp & \aDepCp & \aDepCpAll & \aDepCpI & \aDepCpII & \aDepCpIII & \aDepCpIV & \aDepCpV & \aDepCpVI & \aDepCpVII & \aDepCpVIII \tabularnewline
da & \aDepDa & \aDepDaAll & \aDepDaI & \aDepDaII & \aDepDaIII & \aDepDaIV & \aDepDaV & \aDepDaVI & \aDepDaVII & \aDepDaVIII \tabularnewline
ju & \aDepJu & \aDepJuAll & \aDepJuI & \aDepJuII & \aDepJuIII & \aDepJuIV & \aDepJuV & \aDepJuVI & \aDepJuVII & \aDepJuVIII \tabularnewline
mnr & \aDepMnr & \aDepMnrAll & \aDepMnrI & \aDepMnrII & \aDepMnrIII & \aDepMnrIV & \aDepMnrV & \aDepMnrVI & \aDepMnrVII & \aDepMnrVIII \tabularnewline
mo & \aDepMo & \aDepMoAll & \aDepMoI & \aDepMoII & \aDepMoIII & \aDepMoIV & \aDepMoV & \aDepMoVI & \aDepMoVII & \aDepMoVIII \tabularnewline
nk & \aDepNk & \aDepNkAll & \aDepNkI & \aDepNkII & \aDepNkIII & \aDepNkIV & \aDepNkV & \aDepNkVI & \aDepNkVII & \aDepNkVIII \tabularnewline
oa & \aDepOa & \aDepOaAll & \aDepOaI & \aDepOaII & \aDepOaIII & \aDepOaIV & \aDepOaV & \aDepOaVI & \aDepOaVII & \aDepOaVIII \tabularnewline
oc & \aDepOc & \aDepOcAll & \aDepOcI & \aDepOcII & \aDepOcIII & \aDepOcIV & \aDepOcV & \aDepOcVI & \aDepOcVII & \aDepOcVIII \tabularnewline
pd & \aDepPd & \aDepPdAll & \aDepPdI & \aDepPdII & \aDepPdIII & \aDepPdIV & \aDepPdV & \aDepPdVI & \aDepPdVII & \aDepPdVIII \tabularnewline
pnc & \aDepPnc & \aDepPncAll & \aDepPncI & \aDepPncII & \aDepPncIII & \aDepPncIV & \aDepPncV & \aDepPncVI & \aDepPncVII & \aDepPncVIII \tabularnewline
ROOT & root of sentence & \aDepRootAll & \aDepRootI & \aDepRootII & \aDepRootIII & \aDepRootIV & \aDepRootV & \aDepRootVI & \aDepRootVII & \aDepRootVIII \tabularnewline
sb & \aDepSb & \aDepSbAll & \aDepSbI & \aDepSbII & \aDepSbIII & \aDepSbIV & \aDepSbV & \aDepSbVI & \aDepSbVII & \aDepSbVIII \tabularnewline
svp & separable verb prefix & \aDepSvpAll & \aDepSvpI & \aDepSvpII & \aDepSvpIII & \aDepSvpIV & \aDepSvpV & \aDepSvpVI & \aDepSvpVII & \aDepSvpVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


\subsubsection*{Lemmatization (\texttt{lemma})} The base form (root) of a word.


\subsubsection*{Common Word (\texttt{stop})}
This column's cell provides information if the word is part of a stop list, hence one of the most common words in the German language or not (\texttt{True} vs. \texttt{False}).


\subsubsection*{Word embedding (\texttt{vector})}
% \href{https://en.wikipedia.org/wiki/Word2vec}{word vector}
A 300-dimensional word vector providing a multi-dimensional meaning
representation of a word.
% handling of words unknown to the model
Out-of-vocabulary words with a vector consisting of 300 dimensions of zeroes
were set to \texttt{\#} to save space.


\subsection*{Dataset content}
% \href{https://bids.neuroimaging.io/}{bids.neuroimaging.io} TextGrid format
The annotation comes in two different versions. First, as a text-based TextGrid
file (\texttt{annotation/ fg\_rscut\_ad\_ger\_speech\_tagged.TextGrid}) to be
conveniently edited using the software Praat \citep{boersma2019praat}.
% TSV format
Second, as a text-based, tab-separated-value (TSV) formatted table
(\texttt{annotation/fg\_rscut\_ ad\_ger\_speech\_tagged.tsv}) in accordance with
the brain imaging data structure (\href{https://bids.neuroimaging.io/}{BIDS})
\citep{gorgolewski2016bids}.
% source code for descriptive statistics
The source code for all descriptive statistics included in this paper is
available in \texttt{code/descriptive-statistics.py} (Python script).


\section*{Dataset validation}
% intro
In order to assess the annotation's quality, we investigated if contrasting
speech-related events to events without speech lead to increased activation in
areas known to be involved in language processing \citep{hickok2007cortical}.
% nouns and proper nouns resp. > coordinate junctions
Moreover, we tested if two similar linguistic concepts (proper nouns and nouns)
providing high semantic information contrasted with a concept providing low
semantic information (coordinate conjunctions) lead to increased activation in
congruent brain areas.

% data source data aligned by applying an affine transformation only are
% available in bold_dico_dico7Tad2grpbold7Tad.nii.gz.  data aligned by
% non-linear warping are available in bold_dico_dico7Tad2grpbold7Tad_nl.nii.gz.
We used a dataset providing blood oxygenation level-dependent (BOLD) functional
magnetic resonance imaging (fMRI) data of 20 subjects (age 21–38 years, mean age
26.6 years, 12 male) listening to the 2h audio-description (\unit[7]{Tesla},
\unit[2]{s} repetition time, 3599 volumes, 36 axial slices, thickness
\unit[1.4]{mm}, \unit[1.4 $\times$ 1.4]{mm} in-plane resolution, \unit[224]{mm}
field-of-view) \citep{hanke2014audiomovie}.
% motion correction
Data were already corrected for motion at the scanner computer.
% alignment
Further, individual BOLD time-series were already aligned by non-linear warping
to a study-specific T2*-weighted echo planar imaging (EPI) group template (cf.
\citep{hanke2014audiomovie} for exact details).

% steps done by C.H.
All further steps for the current analysis were carried out using Feat v6.00
(FMRI Expert Analysis Tool)\citep{woolrich2001autocorr} as part of FSL v5.0.9
(FMRIB's Software Library)\citep{smith2004fsl}.
% dropping of subject 10
Data of one participant were dropped to due to invalid distortion correction
during scanning.
% temporal and spatial filter, brain extraction
Data were temporally high-pass filtered (cut-off \unit[150]{s}), spatially
smoothed (Gaussian kernel; \unit[4.0]{mm} FWHM), and the brain was extracted
from surrounding tissue.
% normalization
A grand-mean intensity normalization of the entire 4D dataset was performed by a
single multiplicative factor.


\begin{table*}[t]
    \caption{Overview of events that were used to create the 26 regressors of the GLM analysis.
The respective counts are given for the whole stimulus and the eight segments
that were used during fMRI scanning.
The 20 most often occurring labels from the detailed part-of speech tagging     (\texttt{tag}) were used as such.
Words belonging to all other labels were pooled to \texttt{tag\_other}.
The label \texttt{sentence} contains the end of complete grammatical sentences.
The label \texttt{phones} contains events of the 80 most often occurring
phonemes (phoneme \texttt{n} with N=6053 to phoneme \texttt{IY1} with N=32).
The label \texttt{no-sp} represents moments when no speech was audible.
\texttt{fg\_ad\_lrdiff} (left-right volume difference) and \texttt{fg\_ad\_rms}
(root mean square energy) were compute for and averaged across every movie frame
(\unit[40]{ms}) via Python script.
Events were convolved with FSL's ``Double-Gamma HRF'' to create the regressors.
The correlation of these regressors over the time course of the whole stimulus
can be seen in Figure \ref{fig:reg-corr}.}
\label{tab:regressors}
\footnotesize
\begin{tabular}{lp{3.5cm}lllllllll}
\toprule
\textbf{label} &  \textbf{description} & \textbf{all} & \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} \\
\midrule
adja & \aTagAdja & \rAdjaAll & \rAdjaI & \rAdjaII & \rAdjaIII & \rAdjaIV & \rAdjaV & \rAdjaVI & \rAdjaVII & \rAdjaVIII \tabularnewline
adjd & \aTagAdjd & \rAdjdAll & \rAdjdI & \rAdjdII & \rAdjdIII & \rAdjdIV & \rAdjdV & \rAdjdVI & \rAdjdVII & \rAdjdVIII \tabularnewline
adv & \aTagAdv & \rAdvAll & \rAdvI & \rAdvII & \rAdvIII & \rAdvIV & \rAdvV & \rAdvVI & \rAdvVII & \rAdvVIII \tabularnewline
appr & \aTagAppr & \rApprAll & \rApprI & \rApprII & \rApprIII & \rApprIV & \rApprV & \rApprVI & \rApprVII & \rApprVIII \tabularnewline
apprart & preposition with article & \rApprartAll & \rApprartI & \rApprartII & \rApprartIII & \rApprartIV & \rApprartV & \rApprartVI & \rApprartVII & \rApprartVIII \tabularnewline
art & \aTagArt & \rArtAll & \rArtI & \rArtII & \rArtIII & \rArtIV & \rArtV & \rArtVI & \rArtVII & \rArtVIII \tabularnewline
kon & \aTagKon & \rKonAll & \rKonI & \rKonII & \rKonIII & \rKonIV & \rKonV & \rKonVI & \rKonVII & \rKonVIII \tabularnewline
ne & \aTagNe & \rNeAll & \rNeI & \rNeII & \rNeIII & \rNeIV & \rNeV & \rNeVI & \rNeVII & \rNeVIII \tabularnewline
nn & \aTagNn & \rNnAll & \rNnI & \rNnII & \rNnIII & \rNnIV & \rNnV & \rNnVI & \rNnVII & \rNnVIII \tabularnewline
pds & substituting demonstrative pronoun & \rPdsAll & \rPdsI & \rPdsII & \rPdsIII & \rPdsIV & \rPdsV & \rPdsVI & \rPdsVII & \rPdsVIII \tabularnewline
pis & substituting indefinite pronoun & \rPisAll & \rPisI & \rPisII & \rPisIII & \rPisIV & \rPisV & \rPisVI & \rPisVII & \rPisVIII \tabularnewline
pper & \aTagPper & \rPperAll & \rPperI & \rPperII & \rPperIII & \rPperIV & \rPperV & \rPperVI & \rPperVII & \rPperVIII \tabularnewline
pposat & \aTagPposat & \rPposatAll & \rPposatI & \rPposatII & \rPposatIII & \rPposatIV & \rPposatV & \rPposatVI & \rPposatVII & \rPposatVIII \tabularnewline
prf & reflexive personal pronoun & \rPrfAll & \rPrfI & \rPrfII & \rPrfIII & \rPrfIV & \rPrfV & \rPrfVI & \rPrfVII & \rPrfVIII \tabularnewline
ptkvz & \aTagPtkvz & \rPtkvzAll & \rPtkvzI & \rPtkvzII & \rPtkvzIII & \rPtkvzIV & \rPtkvzV & \rPtkvzVI & \rPtkvzVII & \rPtkvzVIII \tabularnewline
vafin & \aTagVafin & \rVafinAll & \rVafinI & \rVafinII & \rVafinIII & \rVafinIV & \rVafinV & \rVafinVI & \rVafinVII & \rVafinVIII \tabularnewline
vmfin & finite verb, modal & \rVmfinAll & \rVmfinI & \rVmfinII & \rVmfinIII & \rVmfinIV & \rVmfinV & \rVmfinVI & \rVmfinVII & \rVmfinVIII \tabularnewline
vvfin & \aTagVvfin & \rVvfinAll & \rVvfinI & \rVvfinII & \rVvfinIII & \rVvfinIV & \rVvfinV & \rVvfinVI & \rVvfinVII & \rVvfinVIII \tabularnewline
vvinf & \aTagVvinf & \rVvinfAll & \rVvinfI & \rVvinfII & \rVvinfIII & \rVvinfIV & \rVvinfV & \rVvinfVI & \rVvinfVII & \rVvinfVIII \tabularnewline
vvpp & \aTagVvpp & \rVvppAll & \rVvppI & \rVvppII & \rVvppIII & \rVvppIV & \rVvppV & \rVvppVI & \rVvppVII & \rVvppVIII \tabularnewline
tag\_other & all other TAG categories & \rTagotherAll & \rTagotherI & \rTagotherII & \rTagotherIII & \rTagotherIV & \rTagotherV & \rTagotherVI & \rTagotherVII & \rTagotherVIII \tabularnewline
%\hline
sentence & complete grammatical sentences & \rSentenceAll & \rSentenceI & \rSentenceII & \rSentenceIII & \rSentenceIV & \rSentenceV & \rSentenceVI & \rSentenceVII & \rSentenceVIII \tabularnewline
phones & 80 most often occurring phonemes & \rPhonesAll & \rPhonesI & \rPhonesII & \rPhonesIII & \rPhonesIV & \rPhonesV & \rPhonesVI & \rPhonesVII & \rPhonesVIII \tabularnewline
no-sp & no-speech & \rNospAll & \rNospI & \rNospII & \rNospIII & \rNospIV & \rNospV & \rNospVI & \rNospVII & \rNospVIII \tabularnewline
%\hline
fg\_ad\_lrdiff & left-right volume difference & \rFgadlrdiffAll & \rFgadlrdiffI & \rFgadlrdiffII & \rFgadlrdiffIII & \rFgadlrdiffIV & \rFgadlrdiffV & \rFgadlrdiffVI & \rFgadlrdiffVII & \rFgadlrdiffVIII \tabularnewline
fg\_ad\_rms & root mean square & \rFgadrmsAll & \rFgadrmsI & \rFgadrmsII & \rFgadrmsIII & \rFgadrmsIV & \rFgadrmsV & \rFgadrmsVI & \rFgadrmsVII & \rFgadrmsVIII \tabularnewline
\bottomrule
\end{tabular}
\end{table*}


% intro to analysis
We implemented a standard three-level, voxel-wise general linear model (GLM) to
average parameter estimates across the eight stimulus segments, and later across
19 subjects.
% 1st lvl
At the first level analyzing each segment for each subject individually, we
created 26 regressors (see Table \ref{tab:regressors}) based on events drawn
from the annotation.
% part-of-speech labels: labels to events, top 20
The 20 most often occurring detailed part-of-speech labels (\texttt{nn} with
N=\rNnAll\space to \texttt{prf} with N=\rPrfAll) were modeled as boxcar function
from onset to offset of each word.
% path-of-speech labels: remaining labels
The remaining other part-of-speech labels were pooled to a single new label
(\texttt{tag\_other}; N=\rTagotherAll) and modeled as a boxcar function from a
word's onset to offset.
% phonemes
The 80 most often occurring phonemes (\texttt{n} with N=6053 to \texttt{IY1}
with N=32) were pooled to \texttt{phonemes} (N=\rPhonesAll) and modeled as
boxcar function from a phoneme's onset to offset.
% sentence (endings)
The end of each complete grammatical sentence was modeled as an impulse event
(N=\rSentenceAll) to capture variance correlating with sentence comprehension.
% no-speech intro
``No-speech'' events (\texttt{no-sp}; N=\rNospAll) serving as a control
condition were created by the following rationale to allow both a sufficient
amount of events of the no-speech condition and a sufficient pause to events of
the other categories: events were fitted into
intervals without audible speech that lasted at least \unit[3.6]{s}.
% minimum distance to words
Each event of the no-speech condition had to have a minimum distance of
\unit[1.8]{s} to an onsets or offsets of a word, and to each onset to another
event of the no-speech condition.
% length of no-speech events
A length of \unit[70]{ms} was chosen for no-speech events matching the average
length of phonemes.
% low-level nuisance regressors
Lastly, we used continuous bins of information about low-level auditory features
(left-right difference in volume and root mean square energy) that was averaged
across the length of every movie frame (\unit[40]{ms}) to capture variance
correlating with assumed low-level perceptual processes.
% convolving
Time series of events were convolved with FSL's ``Double-Gamma HRF'' as a model
of the hemodynamic response function to create the actual regressors.
% reference to figure ``correlation of regressors''
The Pearson correlation coefficients of the 26 regressors across the time course
of all stimulus segments can be seen in Figure \ref{fig:reg-corr}.
% temporal derivatives
Temporal derivatives were also included in the design matrix to compensate for
regional differences between modeled and actual HRF.
% motion parameters & temporal filtering of design motion parameter are
% filtered, too. See FSL manual: ``All (confound) EVs will be filtered to match
% the processing applied to the input data''
Finally, six motion parameters were used as additional nuisance regressors and
the design was subjected to the same temporal filtering as the BOLD time series.
% t-contrasts
The following three $t$-contrasts were defined: 1) words (all 21
\texttt{tag}-related regressors) > no-speech (\texttt{no-sp}),
% 1b) no-speech > words
2) proper nouns (\texttt{ne}) > coordinate conjunctions (\texttt{kon}), and
% 2b) coordinate conjunctions > proper nouns
3) nouns (\texttt{nn}) > coordinate conjunctions (\texttt{kon}).
% 3b) coordinate conjunctions > nouns

\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/regressor-corr} \caption{Pearson
        correlation coefficients of the 26 regressors used in the analysis to
        validate the annotation. Regressors were created by convolving the
        events with FSL's ``Double-Gamma HRF'' as a model of the hemodynamic
        response function, temporally filtered with the same high-pass filter
        (cut-off \unit[150]{s}) as the BOLD time series, and concatenated across
    runs before computing the correlation.}
    \label{fig:reg-corr}
\end{figure*}


% 2nd lvl
The second-level analysis that averaged contrast estimates across the eight
stimulus segments per subject was carried out using a fixed effects model by
forcing the random effects variance to zero in FLAME (FMRIB's Local Analysis of
Mixed Effects) \citep{beckmann2003general, woolrich2004multilevel}.
% 3rd lvl
The third level analysis which averaged contrast estimates across subjects was
carried out using a mixed-effects model (FLAME stage 1) with automatic outlier deweighting
\citep{woolrich2004multilevel, woolrich2008robust}.
% thresholding
Z (Gaussianised T/F) statistic images were thresholded using clusters determined
by Z>3.4 and a corrected cluster significance threshold of p<.05
\citep{woolrich2008robust}.
% identification of brain regions
Brain regions associated with observed clusters were labeled using the Jülich
Histological Atlas \citep{eickhoff2005toolbox,eickhoff2007assignment} and the
Harvard-Oxford Cortical Atlas \citep{desikan2006automated} provided by FSL.


% Results
Figure \ref{fig:results} depicts the results of the three contrasts (z-threshold
Z>3.4; p<.05 cluster-corrected).
% COPE 1:
The contrast words > no-speech yielded four significant clusters (see Table
\ref{tab:cope1}):
% cluster 1
one left-lateralized cluster spanning from the angular gyrus and inferior
posterior supramarginal gyrus across the superior and middle temporal gyrus,
including parts of Heschl's gyrus and planum temporale.
% cluster 2
A second left cluster in (inferior) frontal regions, including precentral gyrus,
pars opercularis (Brodmann Areal 44; BA44) and pars triangularis (BA45).
% cluster 3
Similarly in the right hemisphere, one cluster spanning from the angular gyrus
across the superior and middle temporal gyrus but including frontal inferior
regions (pars opercularis and pars triangularis).
% cluster 4
A fourth significant cluster is located in the left thalamus.

% COPE 3
The contrast proper nouns > coordinate conjunctions yielded nine significant
clusters (see Table \ref{tab:cope3}):
% cluster 1
one left-lateralized cluster spanning from the angular gyrus across planum
temporale and superior temporal gyrus, partially covering the Heschl's gyrus,
into the anterior middle temporal gyrus.
% cluster 2
A largely congruent but smaller cluster in the right hemisphere.
% cluster 3 & 5, and 4 & 6
Two clusters in posterior cingulate cortex and precuneus of both hemispheres.
% remaining 3 clusters of COPE 3
Three small clusters in the right occipital pole, right Heschl's gyrus and left
superior lateral occipital pole.

% COPE 5
The contrast nouns > coordinate conjunctions yielded four significant clusters
(see Table \ref{tab:cope5}):
% cluster 1 & 2
two clusters that are slightly smaller than the lateral temporal clusters of
contrast nouns > coordinate conjunction.
% variation in cluster 1 & 2 in comparison to COPE 1
In this case, spanning from angular gyrus in the left hemisphere and from planum
temporale in the right hemisphere into the anterior part of superior temporal
cortex.
% cluster 3 & 4
Finally, two small right-lateralized clusters in the right posterior cingulate
gyrus and right precuneus.


\begin{figure*} \centering
    \includegraphics[width=\linewidth]{figures/slicescolorbars}
    \caption{Results of the mixed-effects group-level (N=14) GLM $t$-contrasts
        for the audio-description of the movie ``Forrest Gump''.
        Significant clusters (Z>3.4, p<0.05 cluster-corrected) are overlaid on
        the MNI152 T1-weighted head template (grey).
        Light grey: The audio-description dataset's field-of-view
        (cf. \citep{hanke2014audiomovie}).}
    \label{fig:results}
\end{figure*}


\begin{table*}[t]
\caption{Significant clusters (z-threshold Z>3.4; p<.05 cluster-corrected) for the contrast words (all 21 \texttt{tag}-related regressors) > no-speech.
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope1}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
14990 & <.001 & 6.31 & -49 & -24.7 & 6.35 & -54.8 & -32.5 & 3.73 &
l. Heschl's g.;
lateral superior occipital c., angular g., superior \& middle temporal g. (posterior to anterior);
parts of supramarginal g. \& planum temporale \\
14469 & <.001 & 6.48 & 55 & -14.9 & -6.9 & 54.1 & -23.1 & 0.374 &
r. superior temporal g.;
angular g., superior (and middle) temporal g. (posterior to anterior), Heschl's g.;
parts of supramarginal g., planum temporale, pars opercularis (BA44) \& pars triangularis (BA45) \\
1971 & <.001 & 5.26 & -51.1 & 25.6 & -10.5 & -53.6 & 17.8 & 10.2 & l. frontal orbital c.;
pars opercularis (BA44), pars triangularis (BA45);
parts of precentral g. \\
217 & .002 & 4.55 & -4.48 & -13.7 & 10.3 & -6.46 & -14.9 & 9.96 & l. thalamus \\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\caption{Significant clusters (z-threshold Z>3.4; p<.05 cluster-corrected) for the contrast proper nouns (\texttt{ne}) > coordinate conjunctions (\texttt{kon}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope3}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
7691 & <.001 & 6.23 & -61.2 & -22.3 & 11.6 & -55.9 & -20.7 & 4.03 &
l. planum temporale; posterior inferior supramarginal g., superior temporal g., planum polare,
parts of posterior angular g.,  Heschl's g., middle temporal gyrus \\
5928 & <.001 & 5.5 & 57.5 & -26.2 & 15.9 & 58.2 & -15.8 & 3.55 &
r. planum temporale;
Heschl's g., superior temporal g., planum polare, temporal pole;
parts of angular g. \& posterior inferior supramarginal gyrus \\
479 & <.001 & 4.62 & -5.42 & -32.3 & 25.3 & -4.28 & -39.4 & 22.8 & l. posterior cingulate g. \\
420 & <.001 & 4.85 & -4.76 & -71.4 & 40.1 & -3.74 & -68.5 & 36.2 & l. precuneus \\
407 & <.001 & 5.07 & 6.83 & -40.1 & 24.5 & 6.67 & -38.7 & 23.1 & r. posterior cingulate c. \\
294 & <.001 & 4.57 & 17 & -69.1 & 34.6 & 17.7 & -67.1 & 34.9 & r. precuneus \\
121 & .024 & 3.95 & 8.12 & -98.2 & 0.359 & 8.75 & -97.7 & -3.15 & r. occipital pole \\
117 & .027 & 4.38 & 36.9 & -24.8 & 4.55 & 37.4 & -23 & 3.09 & r. Heschl's g. \\
115 & .029 & 4.08 & -44.6 & -71.7 & 21.7 & -43.6 & -70.8 & 23.4 & l. superior lateral occipital c.\\
\bottomrule
\end{tabular}
\end{table*}


\begin{table*}[t]
\caption{Significant clusters (z-threshold Z>3.4; p<.05 cluster-corrected) for the contrast nouns (\texttt{nn}) > coordinate conjunctions (\texttt{kon}).
Clusters sorted by voxel size.
The first brain structure given contains the voxel with the maximum Z-Value,
followed by brain structures from posterior to anterior, and partially covered
areas (l. = left; r. = right; c. = cortex; g. = gyrus).}
\label{tab:cope5}
\begin{tabular}{rrrrrrrrrp{6cm}}
\toprule
& & & \multicolumn{3}{r}{max location (MNI)} & \multicolumn{3}{r}{center of gravity (MNI)} &
\\ \cmidrule{4-6} \cmidrule{7-9}
voxels & $p_{corr.}$ & Z-max & x & y & z  & x & y & z & structure \\
\midrule
3166 & <.001 & 5.75 & -61.3 & -10.6 & -2.93 & -57.7 & -14.3 & 1.47 &
l. anterior superior (and middle) temporal g.;
planum temporale, planum polare, anterior superior temporal g.;
part of posterior supramarginal g., Heschl's g. \\
1753 & <.001 & 4.99 & 63.3 & -15.1 & 8.41 & 58 & -13 & 4.02 & r. planum temporale, anterior superior temporal g., planum polare;
part of\& part of Heschl's G. \\
166 & .004 & 4.5 & 6.83 & -40.1 & 24.5 & 7.01 & -39.7 & 24.2 &
r. posterior cingulate g. \\
149 & .008 & 4.13 & 18.2 & -67.8 & 36 & 19.8 & -66.4 & 34.6 &
r. precuneus \\
\bottomrule
\end{tabular}
\end{table*}


% Discussion words > no-speech
For the contrast words > no-speech, results show increased hemodynamic activity
in a bilateral cortical network including temporal, parietal and frontal regions
related to processing spoken language \citep{friederici2011brain,
hickok2007cortical,price2012twentyyears}.
% similarities to previous naturalistic speech paradigms
These clusters resemble results of previous studies that implemented an ISC
approach to analyze fMRI data of naturalistic auditory stimuli
\citep{honey2012not, lerner2011topographic, silbert2014coupled}.
% differences
We do not find significantly increased activations in midline areas (like the
posterior cingulate cortex and precuneus or anterior cingulate cortex and medial
frontal cortex) which showed synchronized activity across subjects in previous
studies.
% Wilson (2008): ISC + GLM
In this regard, our results are similar to \citep{wilson2008beyond} who
implemented both an ISC and a GLM analysis. In this study, the ISC analysis
showed synchronized activity in midline areas but the GLM analysis contrasting
blocks of listening to narratives to blocks of a resting condition showed
significantly decreased activity in these areas.

% (proper) nouns > con
The two contrasts that contrasted nouns and proper nouns respectively to
coordinate junctions yielded increased activation partially located in early
sensory regions (Heschl's Gyrus; \citep{saenz2014tonotopic}) and most
prominently adjacent regions bilaterally (planum temporale; superior temporal
gyrus; \citep{arsenault2015distributed, mesgarani2014phonetic}).
% why these contrasts nous & proper nouns
We chose nouns and proper nouns for these two contrasts because they represent
linguistically similar concepts but are uncorrelated in the German language
and stimulus (cf. Figure \ref{fig:reg-corr}).
% coordinate conjunctions
We contrasted nouns and proper nouns respectively to coordinate conjunctions
because nouns and proper nouns are linguistically different to coordinate
conjunctions as well as uncorrelated.
% “interpretation”
Despite the fact that
nouns and proper nouns are uncorrelated, both contrasts lead to largely
spatially congruent clusters. Results suggest that models based on our
annotation of similar linguistic concepts correlate with hemodynamic activity in
spatially similar areas.
% not driven by coordinate conjunction regressors
We confirmed the validity of these interpretation by testing if the spatial
congruency could be attributed to a negative correlation of coordinate
conjunctions with the modeled time series which turned out not to be the case.
% why it's cool
In summary, results of our exploratory analyses suggest that the annotation of
speech meets basic quality requirements to be a basis for model-based
analyses that investigate language perception under more ecologically valid
conditions.


\section*{Data availability}

The annotation (tab-separated values (TSV) formatted table and TextGrid file),
as well as scripts and data of the validation analysis are available from
OpenScienceFramework project
\href{http://dx.doi.org/10.17605/OSF.IO/GFRME}{10.17605/OSF.IO/GFRME}, and
accessible as DataLad (RRID:SCR\_003931) datasets.


\section*{Conclusions}
% what the paper is about
As an extension of the studyforrest dataset, we present an annotation of spoken
language in the two-hours audio-visual movie
``Forrest Gump'' and its temporally aligned audio-description.
% what we did
We annotated the onset and offset of sentences, words and phonemes, and the
corresponding speaker's identity.
% what for
An additional tagging of each word's linguistic features enables creating models
of hemodynamic activity
correlating with linguistics aspects like phonetics, grammar, syntax, and
semantics occurring in two naturalistic stimuli.
% results
Results of our analysis indicate that
the annotation's content can be used to locate speech-related networks
in the human brain under life-like conditions.


\subsection*{Author contributions}
% subsection in Schnitte-Anno
%In order to give appropriate credit to each author of an article, the
%individual contributions of each author to the manuscript should be detailed
%in this section. We recommend using author initials and then stating briefly
%how they contributed.
COH designed, performed, and validated the annotation, and wrote the manuscript.
MH provided critical feedback on the procedure and wrote the manuscript.

\subsection*{Competing interests}
% subsection in Schnitte-Anno
No competing interests were disclosed.

\subsection*{Grant information}
COH was supported by a graduate stipend from the German federal state of
Saxony-Anhalt and MH was supported by funds from the German federal state of
Saxony-Anhalt and the European Regional Development Fund (ERDF), Project:
Center for Behavioral Brain Sciences (CBBS). Work on the adapting data
management technology for this study was in part supported by the European
Union's Horizon 2020 Research and Innovation Programme under Grant Agreement
no. 785907 (HBP SGA2).


\subsection*{Acknowledgements}
COH is grateful to Valeri Kippes who took care of the author's mental sanity by providing excellent training at his gym in Jülich during the mentally draining period of manual corrections of the annotation.

{\small\bibliographystyle{unsrtnat}
\bibliography{references}}

\end{document}
